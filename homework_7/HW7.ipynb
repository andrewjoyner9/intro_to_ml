{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRYwi1zt-8WX",
        "outputId": "698e2706-2d16-4d86-e4fc-e11e0034a642"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300 | Loss: 1.3964\n",
            "Epoch 2/300 | Loss: 1.0250\n",
            "Epoch 3/300 | Loss: 0.8576\n",
            "Epoch 4/300 | Loss: 0.7355\n",
            "Epoch 5/300 | Loss: 0.6287\n",
            "Epoch 6/300 | Loss: 0.5295\n",
            "Epoch 7/300 | Loss: 0.4304\n",
            "Epoch 8/300 | Loss: 0.3396\n",
            "Epoch 9/300 | Loss: 0.2543\n",
            "Epoch 10/300 | Loss: 0.1823\n",
            "Epoch 11/300 | Loss: 0.1260\n",
            "Epoch 12/300 | Loss: 0.0938\n",
            "Epoch 13/300 | Loss: 0.0722\n",
            "Epoch 14/300 | Loss: 0.0635\n",
            "Epoch 15/300 | Loss: 0.0510\n",
            "Epoch 16/300 | Loss: 0.0550\n",
            "Epoch 17/300 | Loss: 0.0506\n",
            "Epoch 18/300 | Loss: 0.0544\n",
            "Epoch 19/300 | Loss: 0.0365\n",
            "Epoch 20/300 | Loss: 0.0376\n",
            "Epoch 21/300 | Loss: 0.0438\n",
            "Epoch 22/300 | Loss: 0.0430\n",
            "Epoch 23/300 | Loss: 0.0323\n",
            "Epoch 24/300 | Loss: 0.0395\n",
            "Epoch 25/300 | Loss: 0.0353\n",
            "Epoch 26/300 | Loss: 0.0196\n",
            "Epoch 27/300 | Loss: 0.0343\n",
            "Epoch 28/300 | Loss: 0.0360\n",
            "Epoch 29/300 | Loss: 0.0355\n",
            "Epoch 30/300 | Loss: 0.0314\n",
            "Epoch 31/300 | Loss: 0.0255\n",
            "Epoch 32/300 | Loss: 0.0365\n",
            "Epoch 33/300 | Loss: 0.0216\n",
            "Epoch 34/300 | Loss: 0.0191\n",
            "Epoch 35/300 | Loss: 0.0352\n",
            "Epoch 36/300 | Loss: 0.0258\n",
            "Epoch 37/300 | Loss: 0.0272\n",
            "Epoch 38/300 | Loss: 0.0170\n",
            "Epoch 39/300 | Loss: 0.0275\n",
            "Epoch 40/300 | Loss: 0.0235\n",
            "Epoch 41/300 | Loss: 0.0355\n",
            "Epoch 42/300 | Loss: 0.0227\n",
            "Epoch 43/300 | Loss: 0.0210\n",
            "Epoch 44/300 | Loss: 0.0238\n",
            "Epoch 45/300 | Loss: 0.0311\n",
            "Epoch 46/300 | Loss: 0.0172\n",
            "Epoch 47/300 | Loss: 0.0194\n",
            "Epoch 48/300 | Loss: 0.0237\n",
            "Epoch 49/300 | Loss: 0.0207\n",
            "Epoch 50/300 | Loss: 0.0216\n",
            "Epoch 51/300 | Loss: 0.0155\n",
            "Epoch 52/300 | Loss: 0.0238\n",
            "Epoch 53/300 | Loss: 0.0283\n",
            "Epoch 54/300 | Loss: 0.0185\n",
            "Epoch 55/300 | Loss: 0.0183\n",
            "Epoch 56/300 | Loss: 0.0106\n",
            "Epoch 57/300 | Loss: 0.0234\n",
            "Epoch 58/300 | Loss: 0.0280\n",
            "Epoch 59/300 | Loss: 0.0210\n",
            "Epoch 60/300 | Loss: 0.0188\n",
            "Epoch 61/300 | Loss: 0.0200\n",
            "Epoch 62/300 | Loss: 0.0229\n",
            "Epoch 63/300 | Loss: 0.0248\n",
            "Epoch 64/300 | Loss: 0.0130\n",
            "Epoch 65/300 | Loss: 0.0196\n",
            "Epoch 66/300 | Loss: 0.0217\n",
            "Epoch 67/300 | Loss: 0.0143\n",
            "Epoch 68/300 | Loss: 0.0138\n",
            "Epoch 69/300 | Loss: 0.0224\n",
            "Epoch 70/300 | Loss: 0.0161\n",
            "Epoch 71/300 | Loss: 0.0156\n",
            "Epoch 72/300 | Loss: 0.0227\n",
            "Epoch 73/300 | Loss: 0.0126\n",
            "Epoch 74/300 | Loss: 0.0083\n",
            "Epoch 75/300 | Loss: 0.0286\n",
            "Epoch 76/300 | Loss: 0.0178\n",
            "Epoch 77/300 | Loss: 0.0223\n",
            "Epoch 78/300 | Loss: 0.0120\n",
            "Epoch 79/300 | Loss: 0.0167\n",
            "Epoch 80/300 | Loss: 0.0189\n",
            "Epoch 81/300 | Loss: 0.0125\n",
            "Epoch 82/300 | Loss: 0.0262\n",
            "Epoch 83/300 | Loss: 0.0119\n",
            "Epoch 84/300 | Loss: 0.0129\n",
            "Epoch 85/300 | Loss: 0.0127\n",
            "Epoch 86/300 | Loss: 0.0215\n",
            "Epoch 87/300 | Loss: 0.0198\n",
            "Epoch 88/300 | Loss: 0.0147\n",
            "Epoch 89/300 | Loss: 0.0137\n",
            "Epoch 90/300 | Loss: 0.0166\n",
            "Epoch 91/300 | Loss: 0.0179\n",
            "Epoch 92/300 | Loss: 0.0183\n",
            "Epoch 93/300 | Loss: 0.0163\n",
            "Epoch 94/300 | Loss: 0.0107\n",
            "Epoch 95/300 | Loss: 0.0181\n",
            "Epoch 96/300 | Loss: 0.0181\n",
            "Epoch 97/300 | Loss: 0.0160\n",
            "Epoch 98/300 | Loss: 0.0116\n",
            "Epoch 99/300 | Loss: 0.0147\n",
            "Epoch 100/300 | Loss: 0.0149\n",
            "Epoch 101/300 | Loss: 0.0152\n",
            "Epoch 102/300 | Loss: 0.0173\n",
            "Epoch 103/300 | Loss: 0.0163\n",
            "Epoch 104/300 | Loss: 0.0131\n",
            "Epoch 105/300 | Loss: 0.0150\n",
            "Epoch 106/300 | Loss: 0.0148\n",
            "Epoch 107/300 | Loss: 0.0212\n",
            "Epoch 108/300 | Loss: 0.0092\n",
            "Epoch 109/300 | Loss: 0.0129\n",
            "Epoch 110/300 | Loss: 0.0117\n",
            "Epoch 111/300 | Loss: 0.0257\n",
            "Epoch 112/300 | Loss: 0.0149\n",
            "Epoch 113/300 | Loss: 0.0109\n",
            "Epoch 114/300 | Loss: 0.0116\n",
            "Epoch 115/300 | Loss: 0.0132\n",
            "Epoch 116/300 | Loss: 0.0205\n",
            "Epoch 117/300 | Loss: 0.0181\n",
            "Epoch 118/300 | Loss: 0.0102\n",
            "Epoch 119/300 | Loss: 0.0102\n",
            "Epoch 120/300 | Loss: 0.0132\n",
            "Epoch 121/300 | Loss: 0.0116\n",
            "Epoch 122/300 | Loss: 0.0181\n",
            "Epoch 123/300 | Loss: 0.0163\n",
            "Epoch 124/300 | Loss: 0.0139\n",
            "Epoch 125/300 | Loss: 0.0191\n",
            "Epoch 126/300 | Loss: 0.0144\n",
            "Epoch 127/300 | Loss: 0.0087\n",
            "Epoch 128/300 | Loss: 0.0085\n",
            "Epoch 129/300 | Loss: 0.0128\n",
            "Epoch 130/300 | Loss: 0.0217\n",
            "Epoch 131/300 | Loss: 0.0167\n",
            "Epoch 132/300 | Loss: 0.0096\n",
            "Epoch 133/300 | Loss: 0.0120\n",
            "Epoch 134/300 | Loss: 0.0132\n",
            "Epoch 135/300 | Loss: 0.0213\n",
            "Epoch 136/300 | Loss: 0.0174\n",
            "Epoch 137/300 | Loss: 0.0089\n",
            "Epoch 138/300 | Loss: 0.0095\n",
            "Epoch 139/300 | Loss: 0.0091\n",
            "Epoch 140/300 | Loss: 0.0185\n",
            "Epoch 141/300 | Loss: 0.0137\n",
            "Epoch 142/300 | Loss: 0.0138\n",
            "Epoch 143/300 | Loss: 0.0159\n",
            "Epoch 144/300 | Loss: 0.0115\n",
            "Epoch 145/300 | Loss: 0.0133\n",
            "Epoch 146/300 | Loss: 0.0133\n",
            "Epoch 147/300 | Loss: 0.0150\n",
            "Epoch 148/300 | Loss: 0.0131\n",
            "Epoch 149/300 | Loss: 0.0167\n",
            "Epoch 150/300 | Loss: 0.0095\n",
            "Epoch 151/300 | Loss: 0.0120\n",
            "Epoch 152/300 | Loss: 0.0112\n",
            "Epoch 153/300 | Loss: 0.0182\n",
            "Epoch 154/300 | Loss: 0.0140\n",
            "Epoch 155/300 | Loss: 0.0098\n",
            "Epoch 156/300 | Loss: 0.0097\n",
            "Epoch 157/300 | Loss: 0.0080\n",
            "Epoch 158/300 | Loss: 0.0188\n",
            "Epoch 159/300 | Loss: 0.0138\n",
            "Epoch 160/300 | Loss: 0.0162\n",
            "Epoch 161/300 | Loss: 0.0160\n",
            "Epoch 162/300 | Loss: 0.0101\n",
            "Epoch 163/300 | Loss: 0.0115\n",
            "Epoch 164/300 | Loss: 0.0114\n",
            "Epoch 165/300 | Loss: 0.0106\n",
            "Epoch 166/300 | Loss: 0.0185\n",
            "Epoch 167/300 | Loss: 0.0112\n",
            "Epoch 168/300 | Loss: 0.0161\n",
            "Epoch 169/300 | Loss: 0.0110\n",
            "Epoch 170/300 | Loss: 0.0100\n",
            "Epoch 171/300 | Loss: 0.0141\n",
            "Epoch 172/300 | Loss: 0.0097\n",
            "Epoch 173/300 | Loss: 0.0167\n",
            "Epoch 174/300 | Loss: 0.0131\n",
            "Epoch 175/300 | Loss: 0.0109\n",
            "Epoch 176/300 | Loss: 0.0168\n",
            "Epoch 177/300 | Loss: 0.0139\n",
            "Epoch 178/300 | Loss: 0.0125\n",
            "Epoch 179/300 | Loss: 0.0102\n",
            "Epoch 180/300 | Loss: 0.0165\n",
            "Epoch 181/300 | Loss: 0.0152\n",
            "Epoch 182/300 | Loss: 0.0101\n",
            "Epoch 183/300 | Loss: 0.0111\n",
            "Epoch 184/300 | Loss: 0.0102\n",
            "Epoch 185/300 | Loss: 0.0103\n",
            "Epoch 186/300 | Loss: 0.0104\n",
            "Epoch 187/300 | Loss: 0.0166\n",
            "Epoch 188/300 | Loss: 0.0117\n",
            "Epoch 189/300 | Loss: 0.0161\n",
            "Epoch 190/300 | Loss: 0.0177\n",
            "Epoch 191/300 | Loss: 0.0087\n",
            "Epoch 192/300 | Loss: 0.0097\n",
            "Epoch 193/300 | Loss: 0.0134\n",
            "Epoch 194/300 | Loss: 0.0088\n",
            "Epoch 195/300 | Loss: 0.0196\n",
            "Epoch 196/300 | Loss: 0.0176\n",
            "Epoch 197/300 | Loss: 0.0166\n",
            "Epoch 198/300 | Loss: 0.0096\n",
            "Epoch 199/300 | Loss: 0.0099\n",
            "Epoch 200/300 | Loss: 0.0082\n",
            "Epoch 201/300 | Loss: 0.0205\n",
            "Epoch 202/300 | Loss: 0.0136\n",
            "Epoch 203/300 | Loss: 0.0144\n",
            "Epoch 204/300 | Loss: 0.0116\n",
            "Epoch 205/300 | Loss: 0.0070\n",
            "Epoch 206/300 | Loss: 0.0100\n",
            "Epoch 207/300 | Loss: 0.0174\n",
            "Epoch 208/300 | Loss: 0.0159\n",
            "Epoch 209/300 | Loss: 0.0151\n",
            "Epoch 210/300 | Loss: 0.0084\n",
            "Epoch 211/300 | Loss: 0.0125\n",
            "Epoch 212/300 | Loss: 0.0084\n",
            "Epoch 213/300 | Loss: 0.0111\n",
            "Epoch 214/300 | Loss: 0.0141\n",
            "Epoch 215/300 | Loss: 0.0137\n",
            "Epoch 216/300 | Loss: 0.0142\n",
            "Epoch 217/300 | Loss: 0.0129\n",
            "Epoch 218/300 | Loss: 0.0089\n",
            "Epoch 219/300 | Loss: 0.0057\n",
            "Epoch 220/300 | Loss: 0.0127\n",
            "Epoch 221/300 | Loss: 0.0157\n",
            "Epoch 222/300 | Loss: 0.0145\n",
            "Epoch 223/300 | Loss: 0.0133\n",
            "Epoch 224/300 | Loss: 0.0129\n",
            "Epoch 225/300 | Loss: 0.0094\n",
            "Epoch 226/300 | Loss: 0.0126\n",
            "Epoch 227/300 | Loss: 0.0111\n",
            "Epoch 228/300 | Loss: 0.0169\n",
            "Epoch 229/300 | Loss: 0.0108\n",
            "Epoch 230/300 | Loss: 0.0166\n",
            "Epoch 231/300 | Loss: 0.0123\n",
            "Epoch 232/300 | Loss: 0.0139\n",
            "Epoch 233/300 | Loss: 0.0180\n",
            "Epoch 234/300 | Loss: 0.0089\n",
            "Epoch 235/300 | Loss: 0.0097\n",
            "Epoch 236/300 | Loss: 0.0109\n",
            "Epoch 237/300 | Loss: 0.0123\n",
            "Epoch 238/300 | Loss: 0.0138\n",
            "Epoch 239/300 | Loss: 0.0103\n",
            "Epoch 240/300 | Loss: 0.0137\n",
            "Epoch 241/300 | Loss: 0.0077\n",
            "Epoch 242/300 | Loss: 0.0168\n",
            "Epoch 243/300 | Loss: 0.0098\n",
            "Epoch 244/300 | Loss: 0.0188\n",
            "Epoch 245/300 | Loss: 0.0101\n",
            "Epoch 246/300 | Loss: 0.0109\n",
            "Epoch 247/300 | Loss: 0.0148\n",
            "Epoch 248/300 | Loss: 0.0137\n",
            "Epoch 249/300 | Loss: 0.0148\n",
            "Epoch 250/300 | Loss: 0.0080\n",
            "Epoch 251/300 | Loss: 0.0126\n",
            "Epoch 252/300 | Loss: 0.0114\n",
            "Epoch 253/300 | Loss: 0.0084\n",
            "Epoch 254/300 | Loss: 0.0162\n",
            "Epoch 255/300 | Loss: 0.0135\n",
            "Epoch 256/300 | Loss: 0.0090\n",
            "Epoch 257/300 | Loss: 0.0140\n",
            "Epoch 258/300 | Loss: 0.0128\n",
            "Epoch 259/300 | Loss: 0.0163\n",
            "Epoch 260/300 | Loss: 0.0135\n",
            "Epoch 261/300 | Loss: 0.0068\n",
            "Epoch 262/300 | Loss: 0.0168\n",
            "Epoch 263/300 | Loss: 0.0108\n",
            "Epoch 264/300 | Loss: 0.0142\n",
            "Epoch 265/300 | Loss: 0.0149\n",
            "Epoch 266/300 | Loss: 0.0069\n",
            "Epoch 267/300 | Loss: 0.0048\n",
            "Epoch 268/300 | Loss: 0.0100\n",
            "Epoch 269/300 | Loss: 0.0149\n",
            "Epoch 270/300 | Loss: 0.0130\n",
            "Epoch 271/300 | Loss: 0.0173\n",
            "Epoch 272/300 | Loss: 0.0120\n",
            "Epoch 273/300 | Loss: 0.0159\n",
            "Epoch 274/300 | Loss: 0.0076\n",
            "Epoch 275/300 | Loss: 0.0106\n",
            "Epoch 276/300 | Loss: 0.0129\n",
            "Epoch 277/300 | Loss: 0.0101\n",
            "Epoch 278/300 | Loss: 0.0139\n",
            "Epoch 279/300 | Loss: 0.0195\n",
            "Epoch 280/300 | Loss: 0.0096\n",
            "Epoch 281/300 | Loss: 0.0116\n",
            "Epoch 282/300 | Loss: 0.0080\n",
            "Epoch 283/300 | Loss: 0.0095\n",
            "Epoch 284/300 | Loss: 0.0154\n",
            "Epoch 285/300 | Loss: 0.0159\n",
            "Epoch 286/300 | Loss: 0.0121\n",
            "Epoch 287/300 | Loss: 0.0112\n",
            "Epoch 288/300 | Loss: 0.0197\n",
            "Epoch 289/300 | Loss: 0.0127\n",
            "Epoch 290/300 | Loss: 0.0102\n",
            "Epoch 291/300 | Loss: 0.0124\n",
            "Epoch 292/300 | Loss: 0.0165\n",
            "Epoch 293/300 | Loss: 0.0087\n",
            "Epoch 294/300 | Loss: 0.0066\n",
            "Epoch 295/300 | Loss: 0.0069\n",
            "Epoch 296/300 | Loss: 0.0117\n",
            "Epoch 297/300 | Loss: 0.0191\n",
            "Epoch 298/300 | Loss: 0.0130\n",
            "Epoch 299/300 | Loss: 0.0126\n",
            "Epoch 300/300 | Loss: 0.0061\n",
            "\n",
            "Total Training Time: 4036.11 seconds\n",
            "Test Accuracy: 70.31%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64 * 8 * 8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "epochs = 300\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTotal Training Time: {training_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8-QCwEIPbq7",
        "outputId": "c8e6f063-17e9-4ee7-eca5-7271773f7678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300 | Loss: 1.4421\n",
            "Epoch 2/300 | Loss: 1.0373\n",
            "Epoch 3/300 | Loss: 0.8517\n",
            "Epoch 4/300 | Loss: 0.7177\n",
            "Epoch 5/300 | Loss: 0.6153\n",
            "Epoch 6/300 | Loss: 0.5193\n",
            "Epoch 7/300 | Loss: 0.4349\n",
            "Epoch 8/300 | Loss: 0.3666\n",
            "Epoch 9/300 | Loss: 0.2914\n",
            "Epoch 10/300 | Loss: 0.2299\n",
            "Epoch 11/300 | Loss: 0.1703\n",
            "Epoch 12/300 | Loss: 0.1397\n",
            "Epoch 13/300 | Loss: 0.1090\n",
            "Epoch 14/300 | Loss: 0.0875\n",
            "Epoch 15/300 | Loss: 0.0906\n",
            "Epoch 16/300 | Loss: 0.0702\n",
            "Epoch 17/300 | Loss: 0.0649\n",
            "Epoch 18/300 | Loss: 0.0655\n",
            "Epoch 19/300 | Loss: 0.0696\n",
            "Epoch 20/300 | Loss: 0.0524\n",
            "Epoch 21/300 | Loss: 0.0589\n",
            "Epoch 22/300 | Loss: 0.0486\n",
            "Epoch 23/300 | Loss: 0.0510\n",
            "Epoch 24/300 | Loss: 0.0539\n",
            "Epoch 25/300 | Loss: 0.0414\n",
            "Epoch 26/300 | Loss: 0.0536\n",
            "Epoch 27/300 | Loss: 0.0427\n",
            "Epoch 28/300 | Loss: 0.0547\n",
            "Epoch 29/300 | Loss: 0.0396\n",
            "Epoch 30/300 | Loss: 0.0502\n",
            "Epoch 31/300 | Loss: 0.0391\n",
            "Epoch 32/300 | Loss: 0.0362\n",
            "Epoch 33/300 | Loss: 0.0347\n",
            "Epoch 34/300 | Loss: 0.0436\n",
            "Epoch 35/300 | Loss: 0.0417\n",
            "Epoch 36/300 | Loss: 0.0397\n",
            "Epoch 37/300 | Loss: 0.0390\n",
            "Epoch 38/300 | Loss: 0.0355\n",
            "Epoch 39/300 | Loss: 0.0352\n",
            "Epoch 40/300 | Loss: 0.0331\n",
            "Epoch 41/300 | Loss: 0.0349\n",
            "Epoch 42/300 | Loss: 0.0473\n",
            "Epoch 43/300 | Loss: 0.0387\n",
            "Epoch 44/300 | Loss: 0.0320\n",
            "Epoch 45/300 | Loss: 0.0299\n",
            "Epoch 46/300 | Loss: 0.0338\n",
            "Epoch 47/300 | Loss: 0.0408\n",
            "Epoch 48/300 | Loss: 0.0273\n",
            "Epoch 49/300 | Loss: 0.0297\n",
            "Epoch 50/300 | Loss: 0.0358\n",
            "Epoch 51/300 | Loss: 0.0289\n",
            "Epoch 52/300 | Loss: 0.0268\n",
            "Epoch 53/300 | Loss: 0.0367\n",
            "Epoch 54/300 | Loss: 0.0347\n",
            "Epoch 55/300 | Loss: 0.0377\n",
            "Epoch 56/300 | Loss: 0.0216\n",
            "Epoch 57/300 | Loss: 0.0206\n",
            "Epoch 58/300 | Loss: 0.0386\n",
            "Epoch 59/300 | Loss: 0.0341\n",
            "Epoch 60/300 | Loss: 0.0315\n",
            "Epoch 61/300 | Loss: 0.0398\n",
            "Epoch 62/300 | Loss: 0.0219\n",
            "Epoch 63/300 | Loss: 0.0277\n",
            "Epoch 64/300 | Loss: 0.0230\n",
            "Epoch 65/300 | Loss: 0.0289\n",
            "Epoch 66/300 | Loss: 0.0295\n",
            "Epoch 67/300 | Loss: 0.0209\n",
            "Epoch 68/300 | Loss: 0.0293\n",
            "Epoch 69/300 | Loss: 0.0372\n",
            "Epoch 70/300 | Loss: 0.0239\n",
            "Epoch 71/300 | Loss: 0.0263\n",
            "Epoch 72/300 | Loss: 0.0350\n",
            "Epoch 73/300 | Loss: 0.0313\n",
            "Epoch 74/300 | Loss: 0.0204\n",
            "Epoch 75/300 | Loss: 0.0229\n",
            "Epoch 76/300 | Loss: 0.0292\n",
            "Epoch 77/300 | Loss: 0.0258\n",
            "Epoch 78/300 | Loss: 0.0301\n",
            "Epoch 79/300 | Loss: 0.0333\n",
            "Epoch 80/300 | Loss: 0.0282\n",
            "Epoch 81/300 | Loss: 0.0193\n",
            "Epoch 82/300 | Loss: 0.0269\n",
            "Epoch 83/300 | Loss: 0.0268\n",
            "Epoch 84/300 | Loss: 0.0262\n",
            "Epoch 85/300 | Loss: 0.0210\n",
            "Epoch 86/300 | Loss: 0.0392\n",
            "Epoch 87/300 | Loss: 0.0246\n",
            "Epoch 88/300 | Loss: 0.0219\n",
            "Epoch 89/300 | Loss: 0.0250\n",
            "Epoch 90/300 | Loss: 0.0281\n",
            "Epoch 91/300 | Loss: 0.0317\n",
            "Epoch 92/300 | Loss: 0.0219\n",
            "Epoch 93/300 | Loss: 0.0312\n",
            "Epoch 94/300 | Loss: 0.0206\n",
            "Epoch 95/300 | Loss: 0.0157\n",
            "Epoch 96/300 | Loss: 0.0217\n",
            "Epoch 97/300 | Loss: 0.0325\n",
            "Epoch 98/300 | Loss: 0.0314\n",
            "Epoch 99/300 | Loss: 0.0203\n",
            "Epoch 100/300 | Loss: 0.0224\n",
            "Epoch 101/300 | Loss: 0.0238\n",
            "Epoch 102/300 | Loss: 0.0348\n",
            "Epoch 103/300 | Loss: 0.0248\n",
            "Epoch 104/300 | Loss: 0.0165\n",
            "Epoch 105/300 | Loss: 0.0341\n",
            "Epoch 106/300 | Loss: 0.0219\n",
            "Epoch 107/300 | Loss: 0.0179\n",
            "Epoch 108/300 | Loss: 0.0182\n",
            "Epoch 109/300 | Loss: 0.0355\n",
            "Epoch 110/300 | Loss: 0.0268\n",
            "Epoch 111/300 | Loss: 0.0340\n",
            "Epoch 112/300 | Loss: 0.0248\n",
            "Epoch 113/300 | Loss: 0.0200\n",
            "Epoch 114/300 | Loss: 0.0197\n",
            "Epoch 115/300 | Loss: 0.0253\n",
            "Epoch 116/300 | Loss: 0.0284\n",
            "Epoch 117/300 | Loss: 0.0206\n",
            "Epoch 118/300 | Loss: 0.0290\n",
            "Epoch 119/300 | Loss: 0.0262\n",
            "Epoch 120/300 | Loss: 0.0160\n",
            "Epoch 121/300 | Loss: 0.0335\n",
            "Epoch 122/300 | Loss: 0.0317\n",
            "Epoch 123/300 | Loss: 0.0129\n",
            "Epoch 124/300 | Loss: 0.0210\n",
            "Epoch 125/300 | Loss: 0.0321\n",
            "Epoch 126/300 | Loss: 0.0257\n",
            "Epoch 127/300 | Loss: 0.0279\n",
            "Epoch 128/300 | Loss: 0.0233\n",
            "Epoch 129/300 | Loss: 0.0251\n",
            "Epoch 130/300 | Loss: 0.0166\n",
            "Epoch 131/300 | Loss: 0.0256\n",
            "Epoch 132/300 | Loss: 0.0180\n",
            "Epoch 133/300 | Loss: 0.0226\n",
            "Epoch 134/300 | Loss: 0.0331\n",
            "Epoch 135/300 | Loss: 0.0240\n",
            "Epoch 136/300 | Loss: 0.0210\n",
            "Epoch 137/300 | Loss: 0.0187\n",
            "Epoch 138/300 | Loss: 0.0240\n",
            "Epoch 139/300 | Loss: 0.0231\n",
            "Epoch 140/300 | Loss: 0.0222\n",
            "Epoch 141/300 | Loss: 0.0231\n",
            "Epoch 142/300 | Loss: 0.0305\n",
            "Epoch 143/300 | Loss: 0.0298\n",
            "Epoch 144/300 | Loss: 0.0179\n",
            "Epoch 145/300 | Loss: 0.0241\n",
            "Epoch 146/300 | Loss: 0.0309\n",
            "Epoch 147/300 | Loss: 0.0263\n",
            "Epoch 148/300 | Loss: 0.0184\n",
            "Epoch 149/300 | Loss: 0.0306\n",
            "Epoch 150/300 | Loss: 0.0229\n",
            "Epoch 151/300 | Loss: 0.0178\n",
            "Epoch 152/300 | Loss: 0.0245\n",
            "Epoch 153/300 | Loss: 0.0254\n",
            "Epoch 154/300 | Loss: 0.0316\n",
            "Epoch 155/300 | Loss: 0.0175\n",
            "Epoch 156/300 | Loss: 0.0264\n",
            "Epoch 157/300 | Loss: 0.0217\n",
            "Epoch 158/300 | Loss: 0.0201\n",
            "Epoch 159/300 | Loss: 0.0227\n",
            "Epoch 160/300 | Loss: 0.0279\n",
            "Epoch 161/300 | Loss: 0.0271\n",
            "Epoch 162/300 | Loss: 0.0229\n",
            "Epoch 163/300 | Loss: 0.0236\n",
            "Epoch 164/300 | Loss: 0.0216\n",
            "Epoch 165/300 | Loss: 0.0231\n",
            "Epoch 166/300 | Loss: 0.0183\n",
            "Epoch 167/300 | Loss: 0.0180\n",
            "Epoch 168/300 | Loss: 0.0312\n",
            "Epoch 169/300 | Loss: 0.0285\n",
            "Epoch 170/300 | Loss: 0.0185\n",
            "Epoch 171/300 | Loss: 0.0268\n",
            "Epoch 172/300 | Loss: 0.0227\n",
            "Epoch 173/300 | Loss: 0.0240\n",
            "Epoch 174/300 | Loss: 0.0280\n",
            "Epoch 175/300 | Loss: 0.0217\n",
            "Epoch 176/300 | Loss: 0.0191\n",
            "Epoch 177/300 | Loss: 0.0292\n",
            "Epoch 178/300 | Loss: 0.0272\n",
            "Epoch 179/300 | Loss: 0.0320\n",
            "Epoch 180/300 | Loss: 0.0218\n",
            "Epoch 181/300 | Loss: 0.0168\n",
            "Epoch 182/300 | Loss: 0.0281\n",
            "Epoch 183/300 | Loss: 0.0299\n",
            "Epoch 184/300 | Loss: 0.0296\n",
            "Epoch 185/300 | Loss: 0.0246\n",
            "Epoch 186/300 | Loss: 0.0284\n",
            "Epoch 187/300 | Loss: 0.0242\n",
            "Epoch 188/300 | Loss: 0.0225\n",
            "Epoch 189/300 | Loss: 0.0202\n",
            "Epoch 190/300 | Loss: 0.0255\n",
            "Epoch 191/300 | Loss: 0.0263\n",
            "Epoch 192/300 | Loss: 0.0247\n",
            "Epoch 193/300 | Loss: 0.0195\n",
            "Epoch 194/300 | Loss: 0.0247\n",
            "Epoch 195/300 | Loss: 0.0248\n",
            "Epoch 196/300 | Loss: 0.0218\n",
            "Epoch 197/300 | Loss: 0.0244\n",
            "Epoch 198/300 | Loss: 0.0241\n",
            "Epoch 199/300 | Loss: 0.0165\n",
            "Epoch 200/300 | Loss: 0.0317\n",
            "Epoch 201/300 | Loss: 0.0273\n",
            "Epoch 202/300 | Loss: 0.0190\n",
            "Epoch 203/300 | Loss: 0.0268\n",
            "Epoch 204/300 | Loss: 0.0163\n",
            "Epoch 205/300 | Loss: 0.0245\n",
            "Epoch 206/300 | Loss: 0.0254\n",
            "Epoch 207/300 | Loss: 0.0276\n",
            "Epoch 208/300 | Loss: 0.0259\n",
            "Epoch 209/300 | Loss: 0.0272\n",
            "Epoch 210/300 | Loss: 0.0264\n",
            "Epoch 211/300 | Loss: 0.0208\n",
            "Epoch 212/300 | Loss: 0.0270\n",
            "Epoch 213/300 | Loss: 0.0228\n",
            "Epoch 214/300 | Loss: 0.0191\n",
            "Epoch 215/300 | Loss: 0.0262\n",
            "Epoch 216/300 | Loss: 0.0249\n",
            "Epoch 217/300 | Loss: 0.0228\n",
            "Epoch 218/300 | Loss: 0.0295\n",
            "Epoch 219/300 | Loss: 0.0213\n",
            "Epoch 220/300 | Loss: 0.0289\n",
            "Epoch 221/300 | Loss: 0.0310\n",
            "Epoch 222/300 | Loss: 0.0252\n",
            "Epoch 223/300 | Loss: 0.0233\n",
            "Epoch 224/300 | Loss: 0.0181\n",
            "Epoch 225/300 | Loss: 0.0292\n",
            "Epoch 226/300 | Loss: 0.0211\n",
            "Epoch 227/300 | Loss: 0.0219\n",
            "Epoch 228/300 | Loss: 0.0250\n",
            "Epoch 229/300 | Loss: 0.0244\n",
            "Epoch 230/300 | Loss: 0.0260\n",
            "Epoch 231/300 | Loss: 0.0375\n",
            "Epoch 232/300 | Loss: 0.0199\n",
            "Epoch 233/300 | Loss: 0.0145\n",
            "Epoch 234/300 | Loss: 0.0329\n",
            "Epoch 235/300 | Loss: 0.0289\n",
            "Epoch 236/300 | Loss: 0.0294\n",
            "Epoch 237/300 | Loss: 0.0220\n",
            "Epoch 238/300 | Loss: 0.0181\n",
            "Epoch 239/300 | Loss: 0.0128\n",
            "Epoch 240/300 | Loss: 0.0199\n",
            "Epoch 241/300 | Loss: 0.0271\n",
            "Epoch 242/300 | Loss: 0.0399\n",
            "Epoch 243/300 | Loss: 0.0187\n",
            "Epoch 244/300 | Loss: 0.0178\n",
            "Epoch 245/300 | Loss: 0.0303\n",
            "Epoch 246/300 | Loss: 0.0267\n",
            "Epoch 247/300 | Loss: 0.0263\n",
            "Epoch 248/300 | Loss: 0.0247\n",
            "Epoch 249/300 | Loss: 0.0212\n",
            "Epoch 250/300 | Loss: 0.0369\n",
            "Epoch 251/300 | Loss: 0.0321\n",
            "Epoch 252/300 | Loss: 0.0193\n",
            "Epoch 253/300 | Loss: 0.0223\n",
            "Epoch 254/300 | Loss: 0.0294\n",
            "Epoch 255/300 | Loss: 0.0276\n",
            "Epoch 256/300 | Loss: 0.0253\n",
            "Epoch 257/300 | Loss: 0.0302\n",
            "Epoch 258/300 | Loss: 0.0197\n",
            "Epoch 259/300 | Loss: 0.0257\n",
            "Epoch 260/300 | Loss: 0.0245\n",
            "Epoch 261/300 | Loss: 0.0276\n",
            "Epoch 262/300 | Loss: 0.0321\n",
            "Epoch 263/300 | Loss: 0.0278\n",
            "Epoch 264/300 | Loss: 0.0243\n",
            "Epoch 265/300 | Loss: 0.0274\n",
            "Epoch 266/300 | Loss: 0.0303\n",
            "Epoch 267/300 | Loss: 0.0260\n",
            "Epoch 268/300 | Loss: 0.0248\n",
            "Epoch 269/300 | Loss: 0.0259\n",
            "Epoch 270/300 | Loss: 0.0234\n",
            "Epoch 271/300 | Loss: 0.0276\n",
            "Epoch 272/300 | Loss: 0.0235\n",
            "Epoch 273/300 | Loss: 0.0335\n",
            "Epoch 274/300 | Loss: 0.0263\n",
            "Epoch 275/300 | Loss: 0.0217\n",
            "Epoch 276/300 | Loss: 0.0214\n",
            "Epoch 277/300 | Loss: 0.0312\n",
            "Epoch 278/300 | Loss: 0.0341\n",
            "Epoch 279/300 | Loss: 0.0175\n",
            "Epoch 280/300 | Loss: 0.0298\n",
            "Epoch 281/300 | Loss: 0.0348\n",
            "Epoch 282/300 | Loss: 0.0260\n",
            "Epoch 283/300 | Loss: 0.0214\n",
            "Epoch 284/300 | Loss: 0.0278\n",
            "Epoch 285/300 | Loss: 0.0232\n",
            "Epoch 286/300 | Loss: 0.0247\n",
            "Epoch 287/300 | Loss: 0.0233\n",
            "Epoch 288/300 | Loss: 0.0333\n",
            "Epoch 289/300 | Loss: 0.0239\n",
            "Epoch 290/300 | Loss: 0.0341\n",
            "Epoch 291/300 | Loss: 0.0326\n",
            "Epoch 292/300 | Loss: 0.0312\n",
            "Epoch 293/300 | Loss: 0.0271\n",
            "Epoch 294/300 | Loss: 0.0286\n",
            "Epoch 295/300 | Loss: 0.0269\n",
            "Epoch 296/300 | Loss: 0.0216\n",
            "Epoch 297/300 | Loss: 0.0238\n",
            "Epoch 298/300 | Loss: 0.0446\n",
            "Epoch 299/300 | Loss: 0.0295\n",
            "Epoch 300/300 | Loss: 0.0206\n",
            "\n",
            "Training Time: 4132.25 seconds\n",
            "Test Accuracy: 74.91%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "class CNN_3Layer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_3Layer, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN_3Layer().to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "epochs = 300\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "train_time = time.time() - start\n",
        "print(f\"\\nTraining Time: {train_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktDk3KNjgqkI",
        "outputId": "fef1024e-af7e-4aac-fdb5-c44b9ed43620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300 | Loss: 1.3548\n",
            "Epoch 2/300 | Loss: 0.9212\n",
            "Epoch 3/300 | Loss: 0.7429\n",
            "Epoch 4/300 | Loss: 0.6382\n",
            "Epoch 5/300 | Loss: 0.5565\n",
            "Epoch 6/300 | Loss: 0.4957\n",
            "Epoch 7/300 | Loss: 0.4454\n",
            "Epoch 8/300 | Loss: 0.4024\n",
            "Epoch 9/300 | Loss: 0.3627\n",
            "Epoch 10/300 | Loss: 0.3266\n",
            "Epoch 11/300 | Loss: 0.2900\n",
            "Epoch 12/300 | Loss: 0.2584\n",
            "Epoch 13/300 | Loss: 0.2330\n",
            "Epoch 14/300 | Loss: 0.2008\n",
            "Epoch 15/300 | Loss: 0.1880\n",
            "Epoch 16/300 | Loss: 0.1590\n",
            "Epoch 17/300 | Loss: 0.1432\n",
            "Epoch 18/300 | Loss: 0.1291\n",
            "Epoch 19/300 | Loss: 0.1175\n",
            "Epoch 20/300 | Loss: 0.1033\n",
            "Epoch 21/300 | Loss: 0.0993\n",
            "Epoch 22/300 | Loss: 0.0939\n",
            "Epoch 23/300 | Loss: 0.0849\n",
            "Epoch 24/300 | Loss: 0.0819\n",
            "Epoch 25/300 | Loss: 0.0720\n",
            "Epoch 26/300 | Loss: 0.0674\n",
            "Epoch 27/300 | Loss: 0.0709\n",
            "Epoch 28/300 | Loss: 0.0675\n",
            "Epoch 29/300 | Loss: 0.0578\n",
            "Epoch 30/300 | Loss: 0.0608\n",
            "Epoch 31/300 | Loss: 0.0606\n",
            "Epoch 32/300 | Loss: 0.0525\n",
            "Epoch 33/300 | Loss: 0.0523\n",
            "Epoch 34/300 | Loss: 0.0520\n",
            "Epoch 35/300 | Loss: 0.0538\n",
            "Epoch 36/300 | Loss: 0.0439\n",
            "Epoch 37/300 | Loss: 0.0444\n",
            "Epoch 38/300 | Loss: 0.0488\n",
            "Epoch 39/300 | Loss: 0.0459\n",
            "Epoch 40/300 | Loss: 0.0351\n",
            "Epoch 41/300 | Loss: 0.0487\n",
            "Epoch 42/300 | Loss: 0.0478\n",
            "Epoch 43/300 | Loss: 0.0394\n",
            "Epoch 44/300 | Loss: 0.0379\n",
            "Epoch 45/300 | Loss: 0.0377\n",
            "Epoch 46/300 | Loss: 0.0370\n",
            "Epoch 47/300 | Loss: 0.0392\n",
            "Epoch 48/300 | Loss: 0.0337\n",
            "Epoch 49/300 | Loss: 0.0375\n",
            "Epoch 50/300 | Loss: 0.0334\n",
            "Epoch 51/300 | Loss: 0.0335\n",
            "Epoch 52/300 | Loss: 0.0371\n",
            "Epoch 53/300 | Loss: 0.0335\n",
            "Epoch 54/300 | Loss: 0.0294\n",
            "Epoch 55/300 | Loss: 0.0315\n",
            "Epoch 56/300 | Loss: 0.0340\n",
            "Epoch 57/300 | Loss: 0.0288\n",
            "Epoch 58/300 | Loss: 0.0277\n",
            "Epoch 59/300 | Loss: 0.0327\n",
            "Epoch 60/300 | Loss: 0.0338\n",
            "Epoch 61/300 | Loss: 0.0285\n",
            "Epoch 62/300 | Loss: 0.0240\n",
            "Epoch 63/300 | Loss: 0.0249\n",
            "Epoch 64/300 | Loss: 0.0296\n",
            "Epoch 65/300 | Loss: 0.0290\n",
            "Epoch 66/300 | Loss: 0.0256\n",
            "Epoch 67/300 | Loss: 0.0232\n",
            "Epoch 68/300 | Loss: 0.0192\n",
            "Epoch 69/300 | Loss: 0.0272\n",
            "Epoch 70/300 | Loss: 0.0288\n",
            "Epoch 71/300 | Loss: 0.0214\n",
            "Epoch 72/300 | Loss: 0.0217\n",
            "Epoch 73/300 | Loss: 0.0262\n",
            "Epoch 74/300 | Loss: 0.0240\n",
            "Epoch 75/300 | Loss: 0.0231\n",
            "Epoch 76/300 | Loss: 0.0187\n",
            "Epoch 77/300 | Loss: 0.0203\n",
            "Epoch 78/300 | Loss: 0.0287\n",
            "Epoch 79/300 | Loss: 0.0233\n",
            "Epoch 80/300 | Loss: 0.0163\n",
            "Epoch 81/300 | Loss: 0.0157\n",
            "Epoch 82/300 | Loss: 0.0258\n",
            "Epoch 83/300 | Loss: 0.0247\n",
            "Epoch 84/300 | Loss: 0.0126\n",
            "Epoch 85/300 | Loss: 0.0245\n",
            "Epoch 86/300 | Loss: 0.0230\n",
            "Epoch 87/300 | Loss: 0.0187\n",
            "Epoch 88/300 | Loss: 0.0171\n",
            "Epoch 89/300 | Loss: 0.0169\n",
            "Epoch 90/300 | Loss: 0.0209\n",
            "Epoch 91/300 | Loss: 0.0186\n",
            "Epoch 92/300 | Loss: 0.0178\n",
            "Epoch 93/300 | Loss: 0.0271\n",
            "Epoch 94/300 | Loss: 0.0182\n",
            "Epoch 95/300 | Loss: 0.0142\n",
            "Epoch 96/300 | Loss: 0.0153\n",
            "Epoch 97/300 | Loss: 0.0184\n",
            "Epoch 98/300 | Loss: 0.0196\n",
            "Epoch 99/300 | Loss: 0.0190\n",
            "Epoch 100/300 | Loss: 0.0135\n",
            "Epoch 101/300 | Loss: 0.0182\n",
            "Epoch 102/300 | Loss: 0.0171\n",
            "Epoch 103/300 | Loss: 0.0102\n",
            "Epoch 104/300 | Loss: 0.0170\n",
            "Epoch 105/300 | Loss: 0.0248\n",
            "Epoch 106/300 | Loss: 0.0194\n",
            "Epoch 107/300 | Loss: 0.0126\n",
            "Epoch 108/300 | Loss: 0.0103\n",
            "Epoch 109/300 | Loss: 0.0123\n",
            "Epoch 110/300 | Loss: 0.0228\n",
            "Epoch 111/300 | Loss: 0.0178\n",
            "Epoch 112/300 | Loss: 0.0116\n",
            "Epoch 113/300 | Loss: 0.0148\n",
            "Epoch 114/300 | Loss: 0.0157\n",
            "Epoch 115/300 | Loss: 0.0122\n",
            "Epoch 116/300 | Loss: 0.0111\n",
            "Epoch 117/300 | Loss: 0.0180\n",
            "Epoch 118/300 | Loss: 0.0228\n",
            "Epoch 119/300 | Loss: 0.0138\n",
            "Epoch 120/300 | Loss: 0.0088\n",
            "Epoch 121/300 | Loss: 0.0064\n",
            "Epoch 122/300 | Loss: 0.0188\n",
            "Epoch 123/300 | Loss: 0.0208\n",
            "Epoch 124/300 | Loss: 0.0127\n",
            "Epoch 125/300 | Loss: 0.0116\n",
            "Epoch 126/300 | Loss: 0.0114\n",
            "Epoch 127/300 | Loss: 0.0149\n",
            "Epoch 128/300 | Loss: 0.0170\n",
            "Epoch 129/300 | Loss: 0.0124\n",
            "Epoch 130/300 | Loss: 0.0141\n",
            "Epoch 131/300 | Loss: 0.0109\n",
            "Epoch 132/300 | Loss: 0.0157\n",
            "Epoch 133/300 | Loss: 0.0094\n",
            "Epoch 134/300 | Loss: 0.0124\n",
            "Epoch 135/300 | Loss: 0.0151\n",
            "Epoch 136/300 | Loss: 0.0116\n",
            "Epoch 137/300 | Loss: 0.0165\n",
            "Epoch 138/300 | Loss: 0.0095\n",
            "Epoch 139/300 | Loss: 0.0060\n",
            "Epoch 140/300 | Loss: 0.0120\n",
            "Epoch 141/300 | Loss: 0.0190\n",
            "Epoch 142/300 | Loss: 0.0130\n",
            "Epoch 143/300 | Loss: 0.0112\n",
            "Epoch 144/300 | Loss: 0.0083\n",
            "Epoch 145/300 | Loss: 0.0086\n",
            "Epoch 146/300 | Loss: 0.0169\n",
            "Epoch 147/300 | Loss: 0.0119\n",
            "Epoch 148/300 | Loss: 0.0116\n",
            "Epoch 149/300 | Loss: 0.0094\n",
            "Epoch 150/300 | Loss: 0.0160\n",
            "Epoch 151/300 | Loss: 0.0104\n",
            "Epoch 152/300 | Loss: 0.0105\n",
            "Epoch 153/300 | Loss: 0.0119\n",
            "Epoch 154/300 | Loss: 0.0115\n",
            "Epoch 155/300 | Loss: 0.0111\n",
            "Epoch 156/300 | Loss: 0.0074\n",
            "Epoch 157/300 | Loss: 0.0121\n",
            "Epoch 158/300 | Loss: 0.0100\n",
            "Epoch 159/300 | Loss: 0.0060\n",
            "Epoch 160/300 | Loss: 0.0153\n",
            "Epoch 161/300 | Loss: 0.0142\n",
            "Epoch 162/300 | Loss: 0.0079\n",
            "Epoch 163/300 | Loss: 0.0065\n",
            "Epoch 164/300 | Loss: 0.0109\n",
            "Epoch 165/300 | Loss: 0.0152\n",
            "Epoch 166/300 | Loss: 0.0113\n",
            "Epoch 167/300 | Loss: 0.0069\n",
            "Epoch 168/300 | Loss: 0.0045\n",
            "Epoch 169/300 | Loss: 0.0126\n",
            "Epoch 170/300 | Loss: 0.0135\n",
            "Epoch 171/300 | Loss: 0.0104\n",
            "Epoch 172/300 | Loss: 0.0069\n",
            "Epoch 173/300 | Loss: 0.0050\n",
            "Epoch 174/300 | Loss: 0.0105\n",
            "Epoch 175/300 | Loss: 0.0129\n",
            "Epoch 176/300 | Loss: 0.0094\n",
            "Epoch 177/300 | Loss: 0.0098\n",
            "Epoch 178/300 | Loss: 0.0111\n",
            "Epoch 179/300 | Loss: 0.0085\n",
            "Epoch 180/300 | Loss: 0.0063\n",
            "Epoch 181/300 | Loss: 0.0098\n",
            "Epoch 182/300 | Loss: 0.0112\n",
            "Epoch 183/300 | Loss: 0.0089\n",
            "Epoch 184/300 | Loss: 0.0082\n",
            "Epoch 185/300 | Loss: 0.0074\n",
            "Epoch 186/300 | Loss: 0.0084\n",
            "Epoch 187/300 | Loss: 0.0136\n",
            "Epoch 188/300 | Loss: 0.0102\n",
            "Epoch 189/300 | Loss: 0.0093\n",
            "Epoch 190/300 | Loss: 0.0092\n",
            "Epoch 191/300 | Loss: 0.0084\n",
            "Epoch 192/300 | Loss: 0.0102\n",
            "Epoch 193/300 | Loss: 0.0074\n",
            "Epoch 194/300 | Loss: 0.0081\n",
            "Epoch 195/300 | Loss: 0.0125\n",
            "Epoch 196/300 | Loss: 0.0110\n",
            "Epoch 197/300 | Loss: 0.0069\n",
            "Epoch 198/300 | Loss: 0.0071\n",
            "Epoch 199/300 | Loss: 0.0069\n",
            "Epoch 200/300 | Loss: 0.0060\n",
            "Epoch 201/300 | Loss: 0.0128\n",
            "Epoch 202/300 | Loss: 0.0070\n",
            "Epoch 203/300 | Loss: 0.0059\n",
            "Epoch 204/300 | Loss: 0.0082\n",
            "Epoch 205/300 | Loss: 0.0146\n",
            "Epoch 206/300 | Loss: 0.0094\n",
            "Epoch 207/300 | Loss: 0.0047\n",
            "Epoch 208/300 | Loss: 0.0062\n",
            "Epoch 209/300 | Loss: 0.0088\n",
            "Epoch 210/300 | Loss: 0.0107\n",
            "Epoch 211/300 | Loss: 0.0055\n",
            "Epoch 212/300 | Loss: 0.0048\n",
            "Epoch 213/300 | Loss: 0.0115\n",
            "Epoch 214/300 | Loss: 0.0079\n",
            "Epoch 215/300 | Loss: 0.0049\n",
            "Epoch 216/300 | Loss: 0.0084\n",
            "Epoch 217/300 | Loss: 0.0107\n",
            "Epoch 218/300 | Loss: 0.0097\n",
            "Epoch 219/300 | Loss: 0.0081\n",
            "Epoch 220/300 | Loss: 0.0071\n",
            "Epoch 221/300 | Loss: 0.0062\n",
            "Epoch 222/300 | Loss: 0.0075\n",
            "Epoch 223/300 | Loss: 0.0103\n",
            "Epoch 224/300 | Loss: 0.0098\n",
            "Epoch 225/300 | Loss: 0.0070\n",
            "Epoch 226/300 | Loss: 0.0037\n",
            "Epoch 227/300 | Loss: 0.0037\n",
            "Epoch 228/300 | Loss: 0.0091\n",
            "Epoch 229/300 | Loss: 0.0106\n",
            "Epoch 230/300 | Loss: 0.0055\n",
            "Epoch 231/300 | Loss: 0.0038\n",
            "Epoch 232/300 | Loss: 0.0104\n",
            "Epoch 233/300 | Loss: 0.0106\n",
            "Epoch 234/300 | Loss: 0.0065\n",
            "Epoch 235/300 | Loss: 0.0061\n",
            "Epoch 236/300 | Loss: 0.0086\n",
            "Epoch 237/300 | Loss: 0.0058\n",
            "Epoch 238/300 | Loss: 0.0068\n",
            "Epoch 239/300 | Loss: 0.0073\n",
            "Epoch 240/300 | Loss: 0.0110\n",
            "Epoch 241/300 | Loss: 0.0082\n",
            "Epoch 242/300 | Loss: 0.0048\n",
            "Epoch 243/300 | Loss: 0.0073\n",
            "Epoch 244/300 | Loss: 0.0058\n",
            "Epoch 245/300 | Loss: 0.0076\n",
            "Epoch 246/300 | Loss: 0.0091\n",
            "Epoch 247/300 | Loss: 0.0066\n",
            "Epoch 248/300 | Loss: 0.0050\n",
            "Epoch 249/300 | Loss: 0.0041\n",
            "Epoch 250/300 | Loss: 0.0064\n",
            "Epoch 251/300 | Loss: 0.0081\n",
            "Epoch 252/300 | Loss: 0.0098\n",
            "Epoch 253/300 | Loss: 0.0069\n",
            "Epoch 254/300 | Loss: 0.0084\n",
            "Epoch 255/300 | Loss: 0.0051\n",
            "Epoch 256/300 | Loss: 0.0026\n",
            "Epoch 257/300 | Loss: 0.0115\n",
            "Epoch 258/300 | Loss: 0.0093\n",
            "Epoch 259/300 | Loss: 0.0048\n",
            "Epoch 260/300 | Loss: 0.0012\n",
            "Epoch 261/300 | Loss: 0.0044\n",
            "Epoch 262/300 | Loss: 0.0103\n",
            "Epoch 263/300 | Loss: 0.0075\n",
            "Epoch 264/300 | Loss: 0.0078\n",
            "Epoch 265/300 | Loss: 0.0066\n",
            "Epoch 266/300 | Loss: 0.0032\n",
            "Epoch 267/300 | Loss: 0.0058\n",
            "Epoch 268/300 | Loss: 0.0079\n",
            "Epoch 269/300 | Loss: 0.0065\n",
            "Epoch 270/300 | Loss: 0.0037\n",
            "Epoch 271/300 | Loss: 0.0038\n",
            "Epoch 272/300 | Loss: 0.0087\n",
            "Epoch 273/300 | Loss: 0.0072\n",
            "Epoch 274/300 | Loss: 0.0064\n",
            "Epoch 275/300 | Loss: 0.0059\n",
            "Epoch 276/300 | Loss: 0.0031\n",
            "Epoch 277/300 | Loss: 0.0050\n",
            "Epoch 278/300 | Loss: 0.0081\n",
            "Epoch 279/300 | Loss: 0.0076\n",
            "Epoch 280/300 | Loss: 0.0061\n",
            "Epoch 281/300 | Loss: 0.0054\n",
            "Epoch 282/300 | Loss: 0.0045\n",
            "Epoch 283/300 | Loss: 0.0057\n",
            "Epoch 284/300 | Loss: 0.0087\n",
            "Epoch 285/300 | Loss: 0.0050\n",
            "Epoch 286/300 | Loss: 0.0082\n",
            "Epoch 287/300 | Loss: 0.0056\n",
            "Epoch 288/300 | Loss: 0.0036\n",
            "Epoch 289/300 | Loss: 0.0056\n",
            "Epoch 290/300 | Loss: 0.0085\n",
            "Epoch 291/300 | Loss: 0.0083\n",
            "Epoch 292/300 | Loss: 0.0062\n",
            "Epoch 293/300 | Loss: 0.0048\n",
            "Epoch 294/300 | Loss: 0.0047\n",
            "Epoch 295/300 | Loss: 0.0065\n",
            "Epoch 296/300 | Loss: 0.0028\n",
            "Epoch 297/300 | Loss: 0.0034\n",
            "Epoch 298/300 | Loss: 0.0108\n",
            "Epoch 299/300 | Loss: 0.0080\n",
            "Epoch 300/300 | Loss: 0.0028\n",
            "\n",
            "Training Time: 7773.73 seconds\n",
            "Test Accuracy: 82.83%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return torch.relu(out)\n",
        "\n",
        "\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet10, self).__init__()\n",
        "\n",
        "        self.in_channels = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.stage1 = self._make_stage(16, num_blocks=4, stride=1)\n",
        "\n",
        "        self.stage2 = self._make_stage(32, num_blocks=3, stride=2)\n",
        "\n",
        "        self.stage3 = self._make_stage(64, num_blocks=3, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, 10)\n",
        "\n",
        "    def _make_stage(self, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        blocks = []\n",
        "        for s in strides:\n",
        "            blocks.append(ResidualBlock(self.in_channels, out_channels, s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return self.fc(out)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\"./data\", train=True, download=True,\n",
        "                                        transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(\"./data\", train=False, download=True,\n",
        "                                       transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet10().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "epochs = 300\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining Time: {training_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hya6k7pMh5WJ"
      },
      "source": [
        "PART 2 B L2 wd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "aAYVFF7zhSDl",
        "outputId": "09acc065-65bb-4e99-a4ce-70a06045f0c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 1.34564039073027\n",
            "2 0.9060933730181526\n",
            "3 0.725706082704427\n",
            "4 0.6275252693754327\n",
            "5 0.5481794509284027\n",
            "6 0.49428434620427963\n",
            "7 0.4411477628342636\n",
            "8 0.3902010237012068\n",
            "9 0.35330462329985235\n",
            "10 0.3136383768390207\n",
            "11 0.28392247318306846\n",
            "12 0.25223422507800713\n",
            "13 0.2237092358300753\n",
            "14 0.19788213929785486\n",
            "15 0.1811166701986052\n",
            "16 0.1627510543674459\n",
            "17 0.14765328166010738\n",
            "18 0.1365276743726962\n",
            "19 0.12381856927119406\n",
            "20 0.11721470100266854\n",
            "21 0.09973119047310804\n",
            "22 0.11097677999064136\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2348141489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2348141489.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2348141489.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mResNet10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,stride=1):\n",
        "        super(ResidualBlock,self).__init__()\n",
        "        self.conv1=nn.Conv2d(in_channels,out_channels,3,stride,1,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(out_channels)\n",
        "        self.conv2=nn.Conv2d(out_channels,out_channels,3,1,1,bias=False)\n",
        "        self.bn2=nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut=nn.Sequential()\n",
        "        if stride!=1 or in_channels!=out_channels:\n",
        "            self.shortcut=nn.Sequential(\n",
        "                nn.Conv2d(in_channels,out_channels,1,stride,bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    def forward(self,x):\n",
        "        out=torch.relu(self.bn1(self.conv1(x)))\n",
        "        out=self.bn2(self.conv2(out))\n",
        "        out+=self.shortcut(x)\n",
        "        return torch.relu(out)\n",
        "\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet10,self).__init__()\n",
        "        self.in_channels=16\n",
        "        self.conv1=nn.Conv2d(3,16,3,1,1,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(16)\n",
        "        self.stage1=self.make_stage(16,4,1)\n",
        "        self.stage2=self.make_stage(32,3,2)\n",
        "        self.stage3=self.make_stage(64,3,2)\n",
        "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc=nn.Linear(64,10)\n",
        "    def make_stage(self,out_channels,num_blocks,stride):\n",
        "        layers=[]\n",
        "        strides=[stride]+[1]*(num_blocks-1)\n",
        "        for s in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels,out_channels,s))\n",
        "            self.in_channels=out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self,x):\n",
        "        out=torch.relu(self.bn1(self.conv1(x)))\n",
        "        out=self.stage1(out)\n",
        "        out=self.stage2(out)\n",
        "        out=self.stage3(out)\n",
        "        out=self.avgpool(out)\n",
        "        out=out.view(out.size(0),-1)\n",
        "        return self.fc(out)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "trainset=torchvision.datasets.CIFAR10('./data',train=True,download=True,transform=transform)\n",
        "testset=torchvision.datasets.CIFAR10('./data',train=False,download=True,transform=transform)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=128,shuffle=True)\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=128,shuffle=False)\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=ResNet10().to(device)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-4)\n",
        "\n",
        "epochs=300\n",
        "start=time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_value=0\n",
        "    for images,labels in trainloader:\n",
        "        images,labels=images.to(device),labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_value+=loss.item()\n",
        "    print(epoch+1,loss_value/len(trainloader))\n",
        "\n",
        "print(time.time()-start)\n",
        "\n",
        "correct=0\n",
        "total=0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,labels in testloader:\n",
        "        images,labels=images.to(device),labels.to(device)\n",
        "        outputs=model(images)\n",
        "        _,predicted=torch.max(outputs,1)\n",
        "        total+=labels.size(0)\n",
        "        correct+=(predicted==labels).sum().item()\n",
        "\n",
        "print(100*correct/total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bRafpIrh9Kw"
      },
      "source": [
        "resnet dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ-GWP-LiAsG",
        "outputId": "33761d05-a8a5-4223-9684-84c34f17e967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 1.4226526712517604\n",
            "2 1.0028401959277784\n",
            "3 0.8169270017567802\n",
            "4 0.699581999608013\n",
            "5 0.6207336869538592\n",
            "6 0.5480870617472607\n",
            "7 0.5006152335030344\n",
            "8 0.4507504332705837\n",
            "9 0.4112901256974701\n",
            "10 0.3708884808642175\n",
            "11 0.3311370444648406\n",
            "12 0.3066494017458328\n",
            "13 0.2700009365444598\n",
            "14 0.24589617331238353\n",
            "15 0.22188837427998442\n",
            "16 0.2061799835518498\n",
            "17 0.18280285209074348\n",
            "18 0.17726460356465387\n",
            "19 0.1569375036203343\n",
            "20 0.14500575695577486\n",
            "21 0.12596051251072712\n",
            "22 0.1192762791024297\n",
            "23 0.11520477810688794\n",
            "24 0.11179160611594424\n",
            "25 0.10666920942113832\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,stride=1):\n",
        "        super(ResidualBlock,self).__init__()\n",
        "        self.conv1=nn.Conv2d(in_channels,out_channels,3,stride,1,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(out_channels)\n",
        "        self.conv2=nn.Conv2d(out_channels,out_channels,3,1,1,bias=False)\n",
        "        self.bn2=nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut=nn.Sequential()\n",
        "        if stride!=1 or in_channels!=out_channels:\n",
        "            self.shortcut=nn.Sequential(\n",
        "                nn.Conv2d(in_channels,out_channels,1,stride,bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    def forward(self,x):\n",
        "        out=torch.relu(self.bn1(self.conv1(x)))\n",
        "        out=self.bn2(self.conv2(out))\n",
        "        out+=self.shortcut(x)\n",
        "        return torch.relu(out)\n",
        "\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet10,self).__init__()\n",
        "        self.in_channels=16\n",
        "        self.conv1=nn.Conv2d(3,16,3,1,1,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(16)\n",
        "        self.stage1=self.make_stage(16,4,1)\n",
        "        self.stage2=self.make_stage(32,3,2)\n",
        "        self.stage3=self.make_stage(64,3,2)\n",
        "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.dropout=nn.Dropout(0.3)\n",
        "        self.fc=nn.Linear(64,10)\n",
        "    def make_stage(self,out_channels,num_blocks,stride):\n",
        "        layers=[]\n",
        "        strides=[stride]+[1]*(num_blocks-1)\n",
        "        for s in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels,out_channels,s))\n",
        "            self.in_channels=out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self,x):\n",
        "        out=torch.relu(self.bn1(self.conv1(x)))\n",
        "        out=self.stage1(out)\n",
        "        out=self.stage2(out)\n",
        "        out=self.stage3(out)\n",
        "        out=self.avgpool(out)\n",
        "        out=out.view(out.size(0),-1)\n",
        "        out=self.dropout(out)\n",
        "        return self.fc(out)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "trainset=torchvision.datasets.CIFAR10('./data',train=True,download=True,transform=transform)\n",
        "testset=torchvision.datasets.CIFAR10('./data',train=False,download=True,transform=transform)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=128,shuffle=True)\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=128,shuffle=False)\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=ResNet10().to(device)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "epochs=300\n",
        "start=time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_value=0\n",
        "    for images,labels in trainloader:\n",
        "        images,labels=images.to(device),labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_value+=loss.item()\n",
        "    print(epoch+1,loss_value/len(trainloader))\n",
        "\n",
        "print(time.time()-start)\n",
        "\n",
        "correct=0\n",
        "total=0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,labels in testloader:\n",
        "        images,labels=images.to(device),labels.to(device)\n",
        "        outputs=model(images)\n",
        "        _,predicted=torch.max(outputs,1)\n",
        "        total+=labels.size(0)\n",
        "        correct+=(predicted==labels).sum().item()\n",
        "\n",
        "print(100*correct/total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMXJKdh5iBwe"
      },
      "source": [
        "resnet data aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wKiVsrp8iEh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "8718e481-ab5f-4c45-fa0e-74ffe8291279"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-761823886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStochasticDepth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_presets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgiou_loss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneralized_box_iou_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv3dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFrozenBatchNorm2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSqueezeExcitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpoolers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiScaleRoIAlign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mps_roi_align\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mps_roi_align\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPSRoIAlign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mps_roi_pool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mps_roi_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPSRoIPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mroi_align\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroi_align\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/ops/roi_align.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_compile_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBroadcastingList2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0maot_compile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/aot_compile.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecompile_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrecompileContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallbackTrigger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mObservedException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2412\u001b[0m }\n\u001b[1;32m   2413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2414\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mhas_triton_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2415\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_triton.py\u001b[0m in \u001b[0;36mhas_triton_package\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhas_triton_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# submodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from .runtime import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mautotune\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/runtime/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mautotuner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAutotuner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeuristics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautotune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheuristics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRedisRemoteCacheBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRemoteCacheBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJITFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKernelInterface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMockTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinterpret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOutOfResources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterpreterError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/runtime/autotuner.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mknobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKernelInterface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJITFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOutOfResources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPTXASError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/knobs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProtocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypedDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibtriton\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetenv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetenv_bool\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,stride=1):\n",
        "        super(ResidualBlock,self).__init__()\n",
        "        self.conv1=nn.Conv2d(in_channels,out_channels,3,stride,1,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(out_channels)\n",
        "        self.conv2=nn.Conv2d(out_channels,out_channels,3,1,1,bias=False)\n",
        "        self.bn2=nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut=nn.Sequential()\n",
        "        if stride!=1 or in_channels!=out_channels:\n",
        "            self.shortcut=nn.Sequential(\n",
        "                nn.Conv2d(in_channels,out_channels,1,stride,bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    def forward(self,x):\n",
        "        out=torch.relu(self.bn1(self.conv1(x)))\n",
        "        out=self.bn2(self.conv2(out))\n",
        "        out+=self.shortcut(x)\n",
        "        return torch.relu(out)\n",
        "\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet10,self).__init__()\n",
        "        self.in_channels=16\n",
        "        self.conv1=nn.Conv2d(3,16,3,1,1,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(16)\n",
        "        self.stage1=self.make_stage(16,4,1)\n",
        "        self.stage2=self.make_stage(32,3,2)\n",
        "        self.stage3=self.make_stage(64,3,2)\n",
        "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc=nn.Linear(64,10)\n",
        "    def make_stage(self,out_channels,num_blocks,stride):\n",
        "        layers=[]\n",
        "        strides=[stride]+[1]*(num_blocks-1)\n",
        "        for s in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels,out_channels,s))\n",
        "            self.in_channels=out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self,x):\n",
        "        out=torch.relu(self.bn1(self.conv1(x)))\n",
        "        out=self.stage1(out)\n",
        "        out=self.stage2(out)\n",
        "        out=self.stage3(out)\n",
        "        out=self.avgpool(out)\n",
        "        out=out.view(out.size(0),-1)\n",
        "        return self.fc(out)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32,padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "trainset=torchvision.datasets.CIFAR10('./data',train=True,download=True,transform=transform)\n",
        "testset=torchvision.datasets.CIFAR10('./data',train=False,download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])\n",
        ")\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=128,shuffle=True)\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=128,shuffle=False)\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=ResNet10().to(device)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "epochs=300\n",
        "start=time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_value=0\n",
        "    for images,labels in trainloader:\n",
        "        images,labels=images.to(device),labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_value+=loss.item()\n",
        "    print(epoch+1,loss_value/len(trainloader))\n",
        "\n",
        "print(time.time()-start)\n",
        "\n",
        "correct=0\n",
        "total=0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,labels in testloader:\n",
        "        images,labels=images.to(device),labels.to(device)\n",
        "        outputs=model(images)\n",
        "        _,predicted=torch.max(outputs,1)\n",
        "        total+=labels.size(0)\n",
        "        correct+=(predicted==labels).sum().item()\n",
        "\n",
        "print(100*correct/total)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}