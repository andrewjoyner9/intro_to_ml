{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PROBLEM 1"
      ],
      "metadata": {
        "id": "uPwVNkfVR6VL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZqjjWHGOiwM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
        "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
        "\n",
        "# normalize\n",
        "t_u_norm = t_u / t_u.max()\n",
        "\n",
        "# model\n",
        "def model(t_u, w1, w2, b):\n",
        "    return w2 * t_u**2 + w1 * t_u + b\n",
        "\n",
        "# loss\n",
        "def loss_fn(t_p, t_c):\n",
        "    return torch.mean((t_p - t_c)**2)\n",
        "\n",
        "# train loop\n",
        "def training_loop(n_epochs, lr, params, t_u, t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        w1, w2, b = params\n",
        "        t_p = model(t_u, w1, w2, b)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            for p in params:\n",
        "                p -= lr * p.grad\n",
        "        for p in params:\n",
        "            p.grad.zero_()\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"epoch {epoch:5d} | loss {loss.item():.6f} | lr {lr}\")\n",
        "    return params\n",
        "\n",
        "# train nonlinear\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "trained_params = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\ntrain lr={lr}\")\n",
        "    w1 = torch.randn(1, requires_grad=True)\n",
        "    w2 = torch.randn(1, requires_grad=True)\n",
        "    b = torch.randn(1, requires_grad=True)\n",
        "    trained_params[lr] = training_loop(5000, lr, (w1, w2, b), t_u_norm, t_c)\n",
        "\n",
        "# linear baseline\n",
        "def linear_model(t_u, w, b):\n",
        "    return w * t_u + b\n",
        "\n",
        "def linear_training_loop(n_epochs, lr, params, t_u, t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        w, b = params\n",
        "        t_p = linear_model(t_u, w, b)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            w -= lr * w.grad\n",
        "            b -= lr * b.grad\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "    return (w, b, loss.item())\n",
        "\n",
        "print(\"\\ntrain linear baseline\")\n",
        "w = torch.randn(1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "w, b, linear_loss = linear_training_loop(5000, 0.01, (w, b), t_u_norm, t_c)\n",
        "print(f\"linear loss {linear_loss:.6f}\")\n",
        "\n",
        "# eval nonlinear\n",
        "nonlinear_losses = {}\n",
        "for lr, (w1, w2, b) in trained_params.items():\n",
        "    final_loss = loss_fn(model(t_u_norm, w1, w2, b), t_c).item()\n",
        "    nonlinear_losses[lr] = final_loss\n",
        "    print(f\"nonlinear lr={lr} | loss {final_loss:.6f}\")\n",
        "\n",
        "best_lr = min(nonlinear_losses, key=nonlinear_losses.get)\n",
        "best_params = trained_params[best_lr]\n",
        "w1_best, w2_best, b_best = best_params\n",
        "print(f\"\\nbest nonlinear lr={best_lr} | loss {nonlinear_losses[best_lr]:.6f}\")\n",
        "\n",
        "# plot\n",
        "t_range = torch.linspace(0, 1, 100)\n",
        "t_c_linear = linear_model(t_range, w, b)\n",
        "t_c_nonlinear = model(t_range, w1_best, w2_best, b_best)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(t_u_norm.detach(), t_c.detach(), 'o', label='data')\n",
        "plt.plot(t_range.detach(), t_c_linear.detach(), '-', label=f'linear ({linear_loss:.2f})')\n",
        "plt.plot(t_range.detach(), t_c_nonlinear.detach(), '-', label=f'nonlinear ({nonlinear_losses[best_lr]:.2f})')\n",
        "plt.xlabel('normalized t_u')\n",
        "plt.ylabel('t_c')\n",
        "plt.legend()\n",
        "plt.title('linear vs nonlinear (normalized)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(t_u, t_c, 'o', label='data')\n",
        "plt.plot(t_range * t_u.max(), t_c_linear.detach(), '-', label='linear')\n",
        "plt.plot(t_range * t_u.max(), t_c_nonlinear.detach(), '-', label='nonlinear')\n",
        "plt.xlabel('t_u')\n",
        "plt.ylabel('t_c')\n",
        "plt.legend()\n",
        "plt.title('linear vs nonlinear (unnormalized)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROBLEM 2"
      ],
      "metadata": {
        "id": "HiYfu9FvR_YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/main/Dataset/Housing.csv\"\n",
        "df = pd.read_csv(url)\n",
        "display(df.head())\n",
        "\n",
        "# encode labels\n",
        "label_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating',\n",
        "              'airconditioning', 'prefarea', 'furnishingstatus']\n",
        "for col in label_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# features and target\n",
        "features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "X = df[features].values\n",
        "y = df['price'].values.reshape(-1, 1)\n",
        "\n",
        "# split train/val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# normalize\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_val_scaled = scaler_X.transform(X_val)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "# tensors\n",
        "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "X_val_t = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_t = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
        "\n",
        "# model\n",
        "def model(X, W, b):\n",
        "    return X @ W + b\n",
        "\n",
        "# loss\n",
        "def loss_fn(y_pred, y_true):\n",
        "    return torch.mean((y_pred - y_true)**2)\n",
        "\n",
        "# train loop\n",
        "def training_loop(n_epochs, lr, params, X_train, y_train, X_val, y_val):\n",
        "    W, b = params\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        y_pred = model(X_train, W, b)\n",
        "        loss = loss_fn(y_pred, y_train)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            W -= lr * W.grad\n",
        "            b -= lr * b.grad\n",
        "            W.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "        with torch.no_grad():\n",
        "            val_pred = model(X_val, W, b)\n",
        "            val_loss = loss_fn(val_pred, y_val)\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"epoch {epoch:5d} | train {loss.item():.6f} | val {val_loss.item():.6f} | lr {lr}\")\n",
        "    return (W, b, val_loss.item())\n",
        "\n",
        "# train models\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "trained_models = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\ntrain lr={lr}\")\n",
        "    torch.manual_seed(0)\n",
        "    W = torch.randn((X_train_t.shape[1], 1), requires_grad=True)\n",
        "    b = torch.randn(1, requires_grad=True)\n",
        "    trained_models[lr] = training_loop(5000, lr, (W, b), X_train_t, y_train_t, X_val_t, y_val_t)\n",
        "\n",
        "# final losses\n",
        "print(\"\\nfinal val losses\")\n",
        "for lr, (_, _, val_loss) in trained_models.items():\n",
        "    print(f\"lr={lr} | val {val_loss:.6f}\")\n",
        "\n",
        "# best model\n",
        "best_lr = min(trained_models, key=lambda lr: trained_models[lr][2])\n",
        "W_best, b_best, best_val_loss = trained_models[best_lr]\n",
        "print(f\"\\nbest lr={best_lr} | val {best_val_loss:.6f}\")\n",
        "\n",
        "# plot\n",
        "with torch.no_grad():\n",
        "    y_pred_best = model(X_val_t, W_best, b_best)\n",
        "    y_pred_best_unscaled = scaler_y.inverse_transform(y_pred_best.numpy())\n",
        "    y_val_unscaled = scaler_y.inverse_transform(y_val_t.numpy())\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(y_val_unscaled, y_pred_best_unscaled, alpha=0.6)\n",
        "plt.plot([y_val_unscaled.min(), y_val_unscaled.max()],\n",
        "         [y_val_unscaled.min(), y_val_unscaled.max()], 'r--')\n",
        "plt.xlabel('actual price')\n",
        "plt.ylabel('predicted price')\n",
        "plt.title(f'best linear model (lr={best_lr})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kOqEC0k_SBwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROBLEM 3"
      ],
      "metadata": {
        "id": "rKgbXstnS3jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import time\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/main/Dataset/Housing.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "label_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating',\n",
        "              'airconditioning', 'prefarea', 'furnishingstatus']\n",
        "for col in label_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# split\n",
        "features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "X = df[features].values\n",
        "y = df['price'].values.reshape(-1, 1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_val_scaled = scaler_X.transform(X_val)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "# tensors\n",
        "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "X_val_t = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_t = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
        "\n",
        "# train fn\n",
        "def train_model(model, X_train, y_train, X_val, y_val, lr=0.01, epochs=200):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        y_pred = model(X_train)\n",
        "        loss = criterion(y_pred, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_pred = model(X_val)\n",
        "            val_loss = criterion(val_pred, y_val)\n",
        "\n",
        "        ss_res = torch.sum((y_val - val_pred)**2)\n",
        "        ss_tot = torch.sum((y_val - torch.mean(y_val))**2)\n",
        "        r2 = 1 - ss_res / ss_tot\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"epoch {epoch:4d} | train loss: {loss.item():.6f} | val loss: {val_loss.item():.6f} | r²: {r2.item():.4f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    return training_time, loss.item(), val_loss.item(), r2.item()\n",
        "\n",
        "# model 1\n",
        "class NN_1Hidden(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, 8)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "print(\"\\nmodel 1: one hidden layer\")\n",
        "model_1 = NN_1Hidden(X_train_t.shape[1])\n",
        "time_1, train_loss_1, val_loss_1, r2_1 = train_model(model_1, X_train_t, y_train_t, X_val_t, y_val_t, lr=0.01, epochs=200)\n",
        "print(f\"\\ntrain time: {time_1:.2f}s | final train loss: {train_loss_1:.6f} | final val loss: {val_loss_1:.6f} | r²: {r2_1:.4f}\")\n",
        "\n",
        "# model 2\n",
        "class NN_3Hidden(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, 8)\n",
        "        self.fc2 = nn.Linear(8, 8)\n",
        "        self.fc3 = nn.Linear(8, 8)\n",
        "        self.fc4 = nn.Linear(8, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "print(\"\\nmodel 2: three hidden layers\")\n",
        "model_3 = NN_3Hidden(X_train_t.shape[1])\n",
        "time_3, train_loss_3, val_loss_3, r2_3 = train_model(model_3, X_train_t, y_train_t, X_val_t, y_val_t, lr=0.01, epochs=200)\n",
        "print(f\"\\ntrain time: {time_3:.2f}s | final train loss: {train_loss_3:.6f} | final val loss: {val_loss_3:.6f} | r²: {r2_3:.4f}\")\n"
      ],
      "metadata": {
        "id": "OdN_EqOfS5Pt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}