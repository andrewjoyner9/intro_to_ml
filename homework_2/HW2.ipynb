{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGpd5Wi83CIz"
      },
      "outputs": [],
      "source": [
        "# ANDREW JOYNER\n",
        "# 801293231\n",
        "# HOMEWORK 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CNKNBoX5soN"
      },
      "source": [
        "# **PROBLEM 1.A**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy3jsPeH3lfv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMyvffJa3lcx"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/main/Dataset/Housing.csv\"\n",
        "df = pd.read_csv(url)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2LLunRu3lZ0"
      },
      "outputs": [],
      "source": [
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhOiBOzY3lXB"
      },
      "outputs": [],
      "source": [
        "# prepare date for linear regression\n",
        "\n",
        "features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "target = 'price'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "print(\"first five feature rows\")\n",
        "display(X[:5])\n",
        "print(\"\\nfirst fice target values:\")\n",
        "display(y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7izqloip3lUk"
      },
      "outputs": [],
      "source": [
        "# 80% train, 20% test no scaling for problem 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #arbitrary random state 42 (got it off the internet)\n",
        "\n",
        "print(\"Raw (unscaled) features; first five rows\")\n",
        "print(X_train[:5])\n",
        "\n",
        "# split for validation set\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nfinal training set: {X_train_final.shape}\")\n",
        "print(f\"validation set: {X_val.shape}\")\n",
        "print(f\"test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vwXAmRv3lR7"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionGD:\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, max_iterations=2000):\n",
        "        self.learning_rate = learning_rate  # step size\n",
        "        self.max_iterations = max_iterations\n",
        "        self.costs_train = []  # training cost\n",
        "        self.costs_val = []    # validation cost\n",
        "\n",
        "    def add_bias(self, X):\n",
        "        \"\"\"adds bias terms to features\"\"\"\n",
        "        return np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "    def compute_cost(self, X, y, theta):\n",
        "        \"\"\"how much cost...\"\"\"\n",
        "        m = X.shape[0]\n",
        "        predictions = X.dot(theta)  # predict using params\n",
        "        cost = (1/(2*m)) * np.sum((predictions - y)**2)  # calculate error\n",
        "        return cost\n",
        "\n",
        "    def compute_gradients(self, X, y, theta):\n",
        "        \"\"\"calculate gradient for eacxh param\"\"\"\n",
        "        m = X.shape[0]\n",
        "        predictions = X.dot(theta)\n",
        "        gradients = (1/m) * X.T.dot(predictions - y)\n",
        "        return gradients\n",
        "\n",
        "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
        "        \"\"\"train model\"\"\"\n",
        "\n",
        "        X_train_bias = self.add_bias(X_train) #add bias\n",
        "        if X_val is not None:\n",
        "            X_val_bias = self.add_bias(X_val)\n",
        "\n",
        "        # init params to zero\n",
        "        n_features = X_train_bias.shape[1]\n",
        "        self.theta = np.zeros(n_features)\n",
        "        print(f\"Initialized {n_features} parameters to zero\")\n",
        "\n",
        "        # training loop\n",
        "        for i in range(self.max_iterations):\n",
        "            # 1.) calculate training cost\n",
        "            train_cost = self.compute_cost(X_train_bias, y_train, self.theta)\n",
        "            self.costs_train.append(train_cost)\n",
        "\n",
        "            # calculate validation cost\n",
        "            if X_val is not None:\n",
        "                val_cost = self.compute_cost(X_val_bias, y_val, self.theta)\n",
        "                self.costs_val.append(val_cost)\n",
        "\n",
        "            # find gradients to impro e model\n",
        "            gradients = self.compute_gradients(X_train_bias, y_train, self.theta)\n",
        "\n",
        "            # iterate in direction of gradient\n",
        "            self.theta = self.theta - self.learning_rate * gradients\n",
        "\n",
        "            # print progress every 200th iteration so we can keep track of model\n",
        "            if i % 200 == 0:\n",
        "                val_info = f\", Validation Cost: {val_cost:.2f}\" if X_val is not None else \"\"\n",
        "                print(f\"Iteration {i:4d}: Training Cost: {train_cost:.2f}{val_info}\")\n",
        "\n",
        "        print(\"training done\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"predict with new data\"\"\"\n",
        "        X_bias = self.add_bias(X)\n",
        "        return X_bias.dot(self.theta)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"calculate r squared score\"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        ss_res = np.sum((y - predictions) ** 2)\n",
        "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
        "        r2 = 1 - (ss_res / ss_tot)\n",
        "        return r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tG2iua3y3lPg"
      },
      "outputs": [],
      "source": [
        "# testing different learning rates\n",
        "learning_rates = [1e-10, 1e-11, 1e-12]  # using the normal 0.1, 0.05, 0.01 returned values too large so I had to make them smaller................. my head hurts so bad\n",
        "models = {}\n",
        "results = {}\n",
        "\n",
        "print(\"Training with small learning rates for unscaled features\")\n",
        "print(\"(Balances numerical stability with learning effectiveness)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nlearning rate: {lr}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # create and train model\n",
        "    model = LinearRegressionGD(learning_rate=lr, max_iterations=2000)\n",
        "    model.fit(X_train_final, y_train_final, X_val, y_val)\n",
        "\n",
        "\n",
        "    models[lr] = model\n",
        "\n",
        "    # evaluate\n",
        "    test_predictions = model.predict(X_test)\n",
        "    train_r2 = model.score(X_train_final, y_train_final)\n",
        "    val_r2 = model.score(X_val, y_val)\n",
        "    test_r2 = model.score(X_test, y_test)\n",
        "\n",
        "    results[lr] = {\n",
        "        'train_r2': train_r2,\n",
        "        'val_r2': val_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'final_train_cost': model.costs_train[-1],\n",
        "        'final_val_cost': model.costs_val[-1]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nresults:\")\n",
        "    print(f\"   training R²: {train_r2:.4f}\")\n",
        "    print(f\"   validation R²: {val_r2:.4f}\")\n",
        "    print(f\"   test R²: {test_r2:.4f}\")\n",
        "    print(f\"   final training cost: {model.costs_train[-1]:.2f}\")\n",
        "    print(f\"   final validation cost: {model.costs_val[-1]:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Glc2UtC-3lM4"
      },
      "outputs": [],
      "source": [
        "# plot training and validation loss for each learning rate\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# plots for each learning rate\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    model = models[lr]\n",
        "\n",
        "    # plot\n",
        "    plt.plot(model.costs_train, label='Training Loss', color='blue', linewidth=2)\n",
        "    plt.plot(model.costs_val, label='Validation Loss', color='red', linewidth=2)\n",
        "\n",
        "    plt.title(f'Learning Rate: {lr}')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Cost (MSE)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Training and Validation Loss Curves for Different Learning Rates',\n",
        "             fontsize=16, y=1.05)\n",
        "plt.show()\n",
        "\n",
        "# determine best learning rate\n",
        "best_lr = min(results.keys(), key=lambda x: results[x]['final_val_cost'])\n",
        "print(f\"\\nbest learning rate: {best_lr}\")\n",
        "print(f\"best model performance:\")\n",
        "for metric, value in results[best_lr].items():\n",
        "    print(f\"   {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AmPyfc3q3lKX"
      },
      "outputs": [],
      "source": [
        "#plot best learning curve\n",
        "best_model = models[best_lr]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# graph with training and validation loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(best_model.costs_train, label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(best_model.costs_val, label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title(f'Best Model: Learning Rate = {best_lr}')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# actual vs model predicted prices\n",
        "test_predictions = best_model.predict(X_test)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test, test_predictions, alpha=0.6, color='purple')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title(f'Actual vs Predicted (R² = {results[best_lr][\"test_r2\"]:.4f})')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# final model parameters\n",
        "print(f\"final model parameters (θ) for learning rate {best_lr}:\")\n",
        "print(f\"   bias (θ₀): {best_model.theta[0]:.4f}\")\n",
        "for i, feature in enumerate(features):\n",
        "    print(f\"   {feature} (θ{i+1}): {best_model.theta[i+1]:.4f}\")\n",
        "\n",
        "# Calculate additional metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "mae = mean_absolute_error(y_test, test_predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"\\n test set performance metrics:\")\n",
        "print(f\"   mean squared error (MSE): {mse:.2f}\")\n",
        "print(f\"   root mean squared rrror (RMSE): {rmse:.2f}\")\n",
        "print(f\"   mean absolute error (MAE): {mae:.2f}\")\n",
        "print(f\"   r-squared (R²): {results[best_lr]['test_r2']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8FNZo3K5w_Q"
      },
      "source": [
        "# **Problem 1.B**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDqA3phB3lHz"
      },
      "outputs": [],
      "source": [
        "# prepare data for problem 1.b\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# define all 11 features for 1.b\n",
        "features_1b = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom',\n",
        "               'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']\n",
        "\n",
        "\n",
        "# create dataframe copy for preprocessing\n",
        "df_1b = df.copy()\n",
        "\n",
        "# convert categorical variables to binary\n",
        "le = LabelEncoder()\n",
        "categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df_1b.columns:\n",
        "        df_1b[col] = le.fit_transform(df_1b[col])\n",
        "\n",
        "\n",
        "# rxtract features and target for 1.b\n",
        "X_1b = df_1b[features_1b].values\n",
        "y_1b = df_1b['price'].values\n",
        "\n",
        "\n",
        "print(\"First five rows of processed features:\")\n",
        "display(pd.DataFrame(X_1b[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p20PKcLq3lEB"
      },
      "outputs": [],
      "source": [
        "# split\n",
        "X_train_1b, X_test_1b, y_train_1b, y_test_1b = train_test_split(X_1b, y_1b, test_size=0.2, random_state=42)\n",
        "\n",
        "# validation\n",
        "X_train_final_1b, X_val_1b, y_train_final_1b, y_val_1b = train_test_split(\n",
        "    X_train_1b, y_train_1b, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Problem 1.b - Training set: {X_train_final_1b.shape}\")\n",
        "print(f\"Problem 1.b - Validation set: {X_val_1b.shape}\")\n",
        "print(f\"Problem 1.b - Test set: {X_test_1b.shape}\")\n",
        "\n",
        "# ditto\n",
        "learning_rates_1b = [1e-10, 1e-11, 1e-12]\n",
        "models_1b = {}\n",
        "results_1b = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROBLEM 1.b: Training with ALL 11 features (NO SCALING)\")\n",
        "print(\"Using small learning rates to balance stability and learning\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for lr in learning_rates_1b:\n",
        "    print(f\"\\nLearning rate: {lr}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # create and train model\n",
        "    model = LinearRegressionGD(learning_rate=lr, max_iterations=2000)\n",
        "    model.fit(X_train_final_1b, y_train_final_1b, X_val_1b, y_val_1b)\n",
        "\n",
        "    models_1b[lr] = model\n",
        "\n",
        "    test_predictions = model.predict(X_test_1b)\n",
        "    train_r2 = model.score(X_train_final_1b, y_train_final_1b)\n",
        "    val_r2 = model.score(X_val_1b, y_val_1b)\n",
        "    test_r2 = model.score(X_test_1b, y_test_1b)\n",
        "\n",
        "    results_1b[lr] = {\n",
        "        'train_r2': train_r2,\n",
        "        'val_r2': val_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'final_train_cost': model.costs_train[-1],\n",
        "        'final_val_cost': model.costs_val[-1]\n",
        "    }\n",
        "\n",
        "    print(f\"results:\")\n",
        "    print(f\"   training R²: {train_r2:.4f}\")\n",
        "    print(f\"   validation R²: {val_r2:.4f}\")\n",
        "    print(f\"   test R²: {test_r2:.4f}\")\n",
        "    print(f\"   final training cost: {model.costs_train[-1]:.2f}\")\n",
        "    print(f\"   final validation cost: {model.costs_val[-1]:.2f}\")\n",
        "\n",
        "\n",
        "# find best learning rate for problem 1.b\n",
        "best_lr_1b = min(results_1b.keys(), key=lambda x: results_1b[x]['final_val_cost'])\n",
        "print(f\"best learning rate for 1.b: {best_lr_1b}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6bAGPa63k9p"
      },
      "outputs": [],
      "source": [
        "# ditto comments from 1.a\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, lr in enumerate(learning_rates_1b):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    model = models_1b[lr]\n",
        "\n",
        "    plt.plot(model.costs_train, label='Training Loss', color='blue', linewidth=2)\n",
        "    plt.plot(model.costs_val, label='Validation Loss', color='red', linewidth=2)\n",
        "\n",
        "    plt.title(f'Learning Rate: {lr}')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Cost (MSE)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Problem 1.b: Training and Validation Loss Curves (11 Features, No Scaling)',\n",
        "             fontsize=16, y=1.05)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AahtyZuxAGyj"
      },
      "source": [
        "# **PROBLEM 2.A**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUFV6gt0AIqV"
      },
      "outputs": [],
      "source": [
        "# 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #arbitrary random state 42 (got it off the internet)\n",
        "\n",
        "# standardize with sklearn.preprocessing StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"scaled features; first five rows\")\n",
        "print(X_train_scaled[:5])\n",
        "\n",
        "# Split training data further to have validation set for plotting\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train_scaled, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nfinal training set: {X_train_final.shape}\")\n",
        "print(f\"validation set: {X_val.shape}\")\n",
        "print(f\"test set: {X_test_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f85dmLPnASts"
      },
      "outputs": [],
      "source": [
        "# test new learning rates\n",
        "learning_rates = [0.1, 0.05, 0.01]\n",
        "models = {}\n",
        "results = {}\n",
        "\n",
        "print(\"we will train models with different learning rates\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nlearning rate: {lr}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # ditto\n",
        "    model = LinearRegressionGD(learning_rate=lr, max_iterations=1000)\n",
        "    model.fit(X_train_final, y_train_final, X_val, y_val)\n",
        "\n",
        "    models[lr] = model\n",
        "\n",
        "    test_predictions = model.predict(X_test_scaled)\n",
        "    train_r2 = model.score(X_train_final, y_train_final)\n",
        "    val_r2 = model.score(X_val, y_val)\n",
        "    test_r2 = model.score(X_test_scaled, y_test)\n",
        "\n",
        "    results[lr] = {\n",
        "        'train_r2': train_r2,\n",
        "        'val_r2': val_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'final_train_cost': model.costs_train[-1],\n",
        "        'final_val_cost': model.costs_val[-1]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nresults:\")\n",
        "    print(f\"   training R²: {train_r2:.4f}\")\n",
        "    print(f\"   validation R²: {val_r2:.4f}\")\n",
        "    print(f\"   test R²: {test_r2:.4f}\")\n",
        "    print(f\"   final training cost: {model.costs_train[-1]:.2f}\")\n",
        "    print(f\"   final validation cost: {model.costs_val[-1]:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M33S97KUAbqv"
      },
      "outputs": [],
      "source": [
        "# plot training and validation loss for each learning rate\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# plots for each learning rate\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    model = models[lr]\n",
        "\n",
        "    # plot\n",
        "    plt.plot(model.costs_train, label='Training Loss', color='blue', linewidth=2)\n",
        "    plt.plot(model.costs_val, label='Validation Loss', color='red', linewidth=2)\n",
        "\n",
        "    plt.title(f'Learning Rate: {lr}')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Cost (MSE)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Training and Validation Loss Curves for Different Learning Rates',\n",
        "             fontsize=16, y=1.05)\n",
        "plt.show()\n",
        "\n",
        "# determine best learning rate\n",
        "best_lr = min(results.keys(), key=lambda x: results[x]['final_val_cost'])\n",
        "print(f\"\\nbest learning rate: {best_lr}\")\n",
        "print(f\"best model performance:\")\n",
        "for metric, value in results[best_lr].items():\n",
        "    print(f\"   {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nol72pGAc64"
      },
      "outputs": [],
      "source": [
        "# ditto\n",
        "best_model = models[best_lr]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(best_model.costs_train, label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(best_model.costs_val, label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title(f'Best Model: Learning Rate = {best_lr}')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "test_predictions = best_model.predict(X_test_scaled)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test, test_predictions, alpha=0.6, color='purple')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title(f'Actual vs Predicted (R² = {results[best_lr][\"test_r2\"]:.4f})')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"final model parameters (θ) for learning rate {best_lr}:\")\n",
        "print(f\"   bias (θ₀): {best_model.theta[0]:.4f}\")\n",
        "for i, feature in enumerate(features):\n",
        "    print(f\"   {feature} (θ{i+1}): {best_model.theta[i+1]:.4f}\")\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "mae = mean_absolute_error(y_test, test_predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"\\n test set performance metrics:\")\n",
        "print(f\"   mean squared error (MSE): {mse:.2f}\")\n",
        "print(f\"   root mean squared rrror (RMSE): {rmse:.2f}\")\n",
        "print(f\"   mean absolute error (MAE): {mae:.2f}\")\n",
        "print(f\"   r-squared (R²): {results[best_lr]['test_r2']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JzeicXVAry3"
      },
      "source": [
        "# **Problem 2.B**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi4DUC-nCGy3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "features_2b = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom',\n",
        "               'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']\n",
        "\n",
        "df_2b = df.copy()\n",
        "\n",
        "le = LabelEncoder()\n",
        "categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df_2b.columns:\n",
        "        df_2b[col] = le.fit_transform(df_2b[col])\n",
        "\n",
        "X_2b = df_2b[features_2b].values\n",
        "y_2b = df_2b['price'].values\n",
        "\n",
        "print(\"first five rows of processed features (pre-scaling):\")\n",
        "display(pd.DataFrame(X_2b[:5], columns=features_2b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjtCYjFvCIFb"
      },
      "outputs": [],
      "source": [
        "# split data and apply scaling\n",
        "X_train_2b, X_test_2b, y_train_2b, y_test_2b = train_test_split(X_2b, y_2b, test_size=0.2, random_state=42)\n",
        "\n",
        "# standardize features with sklearn StandardScaler\n",
        "scaler_2b = StandardScaler()\n",
        "X_train_scaled_2b = scaler_2b.fit_transform(X_train_2b)\n",
        "X_test_scaled_2b = scaler_2b.transform(X_test_2b)\n",
        "\n",
        "print(\"Scaled features (first five rows):\")\n",
        "print(X_train_scaled_2b[:5])\n",
        "\n",
        "# split for validation set\n",
        "X_train_final_2b, X_val_2b, y_train_final_2b, y_val_2b = train_test_split(\n",
        "    X_train_scaled_2b, y_train_2b, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nProblem 2.b - Training set: {X_train_final_2b.shape}\")\n",
        "print(f\"Problem 2.b - Validation set: {X_val_2b.shape}\")\n",
        "print(f\"Problem 2.b - Test set: {X_test_scaled_2b.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhapEdzECS5B"
      },
      "outputs": [],
      "source": [
        "# train models with different learning rates for scaled 11 features\n",
        "learning_rates_2b = [0.1, 0.05, 0.01]\n",
        "models_2b = {}\n",
        "results_2b = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROBLEM 2.b: Training with ALL 11 features WITH SCALING\")\n",
        "print(\"Using standard learning rates for scaled features\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for lr in learning_rates_2b:\n",
        "    print(f\"\\nLearning rate: {lr}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # create and train model\n",
        "    model = LinearRegressionGD(learning_rate=lr, max_iterations=1000)\n",
        "    model.fit(X_train_final_2b, y_train_final_2b, X_val_2b, y_val_2b)\n",
        "\n",
        "    models_2b[lr] = model\n",
        "\n",
        "    # evaluate performance\n",
        "    test_predictions = model.predict(X_test_scaled_2b)\n",
        "    train_r2 = model.score(X_train_final_2b, y_train_final_2b)\n",
        "    val_r2 = model.score(X_val_2b, y_val_2b)\n",
        "    test_r2 = model.score(X_test_scaled_2b, y_test_2b)\n",
        "\n",
        "    results_2b[lr] = {\n",
        "        'train_r2': train_r2,\n",
        "        'val_r2': val_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'final_train_cost': model.costs_train[-1],\n",
        "        'final_val_cost': model.costs_val[-1]\n",
        "    }\n",
        "\n",
        "    print(f\"Results:\")\n",
        "    print(f\"   Training R²: {train_r2:.4f}\")\n",
        "    print(f\"   Validation R²: {val_r2:.4f}\")\n",
        "    print(f\"   Test R²: {test_r2:.4f}\")\n",
        "    print(f\"   Final training cost: {model.costs_train[-1]:.2f}\")\n",
        "    print(f\"   Final validation cost: {model.costs_val[-1]:.2f}\")\n",
        "\n",
        "# find best learning rate for problem 2.b\n",
        "best_lr_2b = min(results_2b.keys(), key=lambda x: results_2b[x]['final_val_cost'])\n",
        "print(f\"\\nBest learning rate for 2.b: {best_lr_2b}\")\n",
        "print(f\"Best model performance:\")\n",
        "for metric, value in results_2b[best_lr_2b].items():\n",
        "    print(f\"   {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r_kJjgMCWpC"
      },
      "outputs": [],
      "source": [
        "# plot training and validation loss curves for problem 2.b\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, lr in enumerate(learning_rates_2b):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    model = models_2b[lr]\n",
        "\n",
        "    plt.plot(model.costs_train, label='Training Loss', color='blue', linewidth=2)\n",
        "    plt.plot(model.costs_val, label='Validation Loss', color='red', linewidth=2)\n",
        "\n",
        "    plt.title(f'Learning Rate: {lr}')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Cost (MSE)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Problem 2.b: Training and Validation Loss Curves (11 Features, WITH Scaling)',\n",
        "             fontsize=16, y=1.05)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dASmoGMCZDA"
      },
      "outputs": [],
      "source": [
        "# detailed analysis of best model for problem 2.b\n",
        "best_model_2b = models_2b[best_lr_2b]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# plot learning curves\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(best_model_2b.costs_train, label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(best_model_2b.costs_val, label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title(f'Best Model: Learning Rate = {best_lr_2b}')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# actual vs predicted scatter plot\n",
        "test_predictions_2b = best_model_2b.predict(X_test_scaled_2b)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test_2b, test_predictions_2b, alpha=0.6, color='purple')\n",
        "plt.plot([y_test_2b.min(), y_test_2b.max()], [y_test_2b.min(), y_test_2b.max()], 'r--', linewidth=2)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title(f'Actual vs Predicted (R² = {results_2b[best_lr_2b][\"test_r2\"]:.4f})')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# display final model parameters\n",
        "print(f\"Final model parameters (θ) for learning rate {best_lr_2b}:\")\n",
        "print(f\"   Bias (θ₀): {best_model_2b.theta[0]:.4f}\")\n",
        "for i, feature in enumerate(features_2b):\n",
        "    print(f\"   {feature} (θ{i+1}): {best_model_2b.theta[i+1]:.4f}\")\n",
        "\n",
        "# calculate additional performance metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "mse_2b = mean_squared_error(y_test_2b, test_predictions_2b)\n",
        "mae_2b = mean_absolute_error(y_test_2b, test_predictions_2b)\n",
        "rmse_2b = np.sqrt(mse_2b)\n",
        "\n",
        "print(f\"\\nTest set performance metrics:\")\n",
        "print(f\"   Mean Squared Error (MSE): {mse_2b:.2f}\")\n",
        "print(f\"   Root Mean Squared Error (RMSE): {rmse_2b:.2f}\")\n",
        "print(f\"   Mean Absolute Error (MAE): {mae_2b:.2f}\")\n",
        "print(f\"   R-squared (R²): {results_2b[best_lr_2b]['test_r2']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgU7gCGC7NO"
      },
      "source": [
        "# **Problem 3.A**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTxBu6gfCb8P"
      },
      "outputs": [],
      "source": [
        "# new linear regression with linearization\n",
        "class LinearRegressionGDRidge(LinearRegressionGD):\n",
        "    def __init__(self, learning_rate=0.01, max_iterations=2000, lambda_=0.1):\n",
        "        super().__init__(learning_rate, max_iterations)\n",
        "        self.lambda_ = lambda_\n",
        "\n",
        "    def compute_cost(self, X, y, theta, regularize=True):\n",
        "        m = X.shape[0]\n",
        "        predictions = X.dot(theta)\n",
        "        cost = (1/(2*m)) * np.sum((predictions - y)**2)\n",
        "        if regularize:\n",
        "            cost += (self.lambda_/(2*m)) * np.sum(theta[1:]**2)\n",
        "        return cost\n",
        "\n",
        "    def compute_gradients(self, X, y, theta, regularize=True):\n",
        "        m = X.shape[0]\n",
        "        predictions = X.dot(theta)\n",
        "        gradients = (1/m) * X.T.dot(predictions - y)\n",
        "        if regularize:\n",
        "            reg = np.concatenate([[0], self.lambda_ * theta[1:]/m])\n",
        "            gradients += reg\n",
        "        return gradients\n",
        "\n",
        "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
        "        X_train_bias = self.add_bias(X_train)\n",
        "        if X_val is not None:\n",
        "            X_val_bias = self.add_bias(X_val)\n",
        "        n_features = X_train_bias.shape[1]\n",
        "        self.theta = np.zeros(n_features)\n",
        "        self.costs_train = []\n",
        "        self.costs_val = []\n",
        "        for i in range(self.max_iterations):\n",
        "            train_cost = self.compute_cost(X_train_bias, y_train, self.theta, regularize=True)\n",
        "            self.costs_train.append(train_cost)\n",
        "            if X_val is not None:\n",
        "                val_cost = self.compute_cost(X_val_bias, y_val, self.theta, regularize=False)\n",
        "                self.costs_val.append(val_cost)\n",
        "            gradients = self.compute_gradients(X_train_bias, y_train, self.theta, regularize=True)\n",
        "            self.theta = self.theta - self.learning_rate * gradients\n",
        "            if i % 200 == 0:\n",
        "                val_info = f\", Validation Cost: {val_cost:.2f}\" if X_val is not None else \"\"\n",
        "                print(f\"Iteration {i:4d}: Training Cost: {train_cost:.2f}{val_info}\")\n",
        "        print(\"training done (ridge)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XhgTj2LJEsB"
      },
      "outputs": [],
      "source": [
        "ridge_learning_rate = best_lr  # from 2.a\n",
        "ridge_lambda = 0.1\n",
        "\n",
        "ridge_model = LinearRegressionGDRidge(learning_rate=ridge_learning_rate, max_iterations=1000, lambda_=ridge_lambda)\n",
        "ridge_model.fit(X_train_final, y_train_final, X_val, y_val)\n",
        "\n",
        "# eval\n",
        "ridge_test_predictions = ridge_model.predict(X_test_scaled)\n",
        "ridge_train_r2 = ridge_model.score(X_train_final, y_train_final)\n",
        "ridge_val_r2 = ridge_model.score(X_val, y_val)\n",
        "ridge_test_r2 = ridge_model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"\\nRidge Regression Results (3.a):\")\n",
        "print(f\"   Training R²: {ridge_train_r2:.4f}\")\n",
        "print(f\"   Validation R²: {ridge_val_r2:.4f}\")\n",
        "print(f\"   Test R²: {ridge_test_r2:.4f}\")\n",
        "print(f\"   Final training cost: {ridge_model.costs_train[-1]:.2f}\")\n",
        "print(f\"   Final validation cost: {ridge_model.costs_val[-1]:.2f}\")\n",
        "\n",
        "# plot training and validation loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(ridge_model.costs_train, label='Training Loss', color='blue')\n",
        "plt.plot(ridge_model.costs_val, label='Validation Loss', color='red')\n",
        "plt.title('Ridge Regression (3.a): Training and Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# actual vs predicted\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(y_test, ridge_test_predictions, alpha=0.6, color='purple')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title(f'Actual vs Predicted (R² = {ridge_test_r2:.4f})')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n_1Rp-IJnAi"
      },
      "source": [
        "# **Problem 3.B**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6FHTOU_JHyo"
      },
      "outputs": [],
      "source": [
        "ridge_learning_rate_2b = best_lr_2b  # from 2.b\n",
        "ridge_lambda = 0.1\n",
        "\n",
        "ridge_model_2b = LinearRegressionGDRidge(learning_rate=ridge_learning_rate_2b, max_iterations=1000, lambda_=ridge_lambda)\n",
        "ridge_model_2b.fit(X_train_final_2b, y_train_final_2b, X_val_2b, y_val_2b)\n",
        "\n",
        "# evaluate\n",
        "ridge_test_predictions_2b = ridge_model_2b.predict(X_test_scaled_2b)\n",
        "ridge_train_r2_2b = ridge_model_2b.score(X_train_final_2b, y_train_final_2b)\n",
        "ridge_val_r2_2b = ridge_model_2b.score(X_val_2b, y_val_2b)\n",
        "ridge_test_r2_2b = ridge_model_2b.score(X_test_scaled_2b, y_test_2b)\n",
        "\n",
        "print(f\"\\nRidge Regression Results (3.b):\")\n",
        "print(f\"   Training R²: {ridge_train_r2_2b:.4f}\")\n",
        "print(f\"   Validation R²: {ridge_val_r2_2b:.4f}\")\n",
        "print(f\"   Test R²: {ridge_test_r2_2b:.4f}\")\n",
        "print(f\"   Final training cost: {ridge_model_2b.costs_train[-1]:.2f}\")\n",
        "print(f\"   Final validation cost: {ridge_model_2b.costs_val[-1]:.2f}\")\n",
        "\n",
        "# plot training and validation loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(ridge_model_2b.costs_train, label='Training Loss', color='blue')\n",
        "plt.plot(ridge_model_2b.costs_val, label='Validation Loss', color='red')\n",
        "plt.title('Ridge Regression (3.b): Training and Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# avctual vs predicted\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(y_test_2b, ridge_test_predictions_2b, alpha=0.6, color='purple')\n",
        "plt.plot([y_test_2b.min(), y_test_2b.max()], [y_test_2b.min(), y_test_2b.max()], 'r--', linewidth=2)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title(f'Actual vs Predicted (R² = {ridge_test_r2_2b:.4f})')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
